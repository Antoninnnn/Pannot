{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25fa69f-5dc1-45a2-a410-f7e5c4813012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ml Anaconda3\n",
    "!source activate pannot-dev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb01884-7056-4d7f-ae0d-29d82e2ee490",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpannot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpannot_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PannotLlamaForCausalLM\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from pannot.model.language_model.pannot_llama import PannotLlamaForCausalLM\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pannot.mm_utils import (\n",
    "    load_structure_from_pkl,\n",
    "\n",
    "    tokenizer_protein_token,     # similar to tokenizer_image_token\n",
    "    get_model_name_from_path      # if reused or overridden in pannot\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pannot.model.builder import load_pretrained_model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5eb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/scratch/user/yining_yang/TAMU/PhD/Pannot/checkpoints/pannot-Meta-Llama-3.1-8B-Instruct-finetune-lora-v00\"\n",
    "model_base = \"/scratch/user/yining_yang/TAMU/PhD/Pannot/checkpoints/pannot-Meta-Llama-3.1-8B-Instruct-pretrain-v00/checkpoint-24000\"\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, _, context_len = load_pretrained_model(\n",
    "        model_path, model_base, model_name, use_flash_attn=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f615791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model_device = next(model.parameters()).device\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "if hasattr(model, 'get_seq_tower'):\n",
    "        model.get_seq_tower().to(model_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f665405",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_ids = [tokenizer.convert_tokens_to_ids(t) for t in [\"<|end_of_text|>\", \"<|eom_id|>\", \"<|eot_id|>\"] if tokenizer.convert_tokens_to_ids(t) is not None]\n",
    "pad_token_id = tokenizer.pad_token_id or eos_token_ids[0]\n",
    "\n",
    "stopping_keywords = [\"<|end_of_text|>\", \"<|eom_id|>\", \"<|eot_id|>\"]\n",
    "\n",
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d69658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    StoppingCriteria\n",
    ")\n",
    "\n",
    "from pannot.eval.eval_opi.eval_opi_dataset_pannot import KeywordsStoppingCriteria\n",
    "\n",
    "stopping_criteria = [KeywordsStoppingCriteria(stopping_keywords, tokenizer, input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baedad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.2\n",
    "top_p = 0.75\n",
    "num_beams = 1\n",
    "max_new_tokens = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798dae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "                    inputs=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    seqs=seqs,\n",
    "                    seq_attention_mask=seq_attention_masks,\n",
    "                    do_sample=temperature > 0,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    num_beams=num_beams,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                    eos_token_id=eos_token_ids,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    stopping_criteria=stopping_criteria\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a14fa",
   "metadata": {},
   "source": [
    "The data fprm of OPI test sample:\n",
    "\n",
    "{\"id\": \"function_test_3\", \"name\": \"Protein Function Description\", \"instruction\": \"What is the functional description of the protein sequence?\", \"instances\": [{\"input\": \"MSTEGGGRRCQAQVSRRISFSASHRLYSKFLSDEENLKLFGKCNNPNGHGHNYKVVVTVHGEIDPATGMVMNLADLKKYMEEAIMQPLDHKNLDMDVPYFADVVSTTENVAVYIWDNLQKVLPVGVLYKVKVYETDNNIVVYKGE\", \"output\": \"Involved in the biosynthesis of tetrahydrobiopterin, an essential cofactor of aromatic amino acid hydroxylases. Catalyzes the transformation of 7,8-dihydroneopterin triphosphate into 6-pyruvoyl tetrahydropterin.\"}]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702677d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e20c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"What is the functional description of the protein sequence?\"\n",
    "sequence=\"MSTEGGGRRCQAQVSRRISFSASHRLYSKFLSDEENLKLFGKCNNPNGHGHNYKVVVTVHGEIDPATGMVMNLADLKKYMEEAIMQPLDHKNLDMDVPYFADVVSTTENVAVYIWDNLQKVLPVGVLYKVKVYETDNNIVVYKGE\"\n",
    "\n",
    "target = \"Involved in the biosynthesis of tetrahydrobiopterin, an essential cofactor of aromatic amino acid hydroxylases. Catalyzes the transformation of 7,8-dihydroneopterin triphosphate into 6-pyruvoyl tetrahydropterin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROT_TOKEN_INDEX = -300\n",
    "DEFAULT_PROT_TOKEN = \"<prot>\"\n",
    "DEFAULT_PROT_PATCH_TOKEN = \"<prot_patch>\"\n",
    "DEFAULT_PROT_START_TOKEN = \"<prot_start>\"\n",
    "DEFAULT_PROT_END_TOKEN = \"<prot_end>\"\n",
    "PROT_PLACEHOLDER = \"<prot-placeholder>\"\n",
    "\n",
    "SEQ_TOKEN_INDEX = -330\n",
    "DEFAULT_SEQ_TOKEN = \"<seq>\"\n",
    "DEFAULT_SEQ_PATCH_TOKEN = \"<seq_patch>\"\n",
    "DEFAULT_SEQ_START_TOKEN = \"<seq_start>\"\n",
    "DEFAULT_SEQ_END_TOKEN = \"<seq_end>\"\n",
    "\n",
    "SEQUENCE_PLACEHOLDER = '<seq-placeholder>'\n",
    "\n",
    "seq_token_se = DEFAULT_SEQ_START_TOKEN + DEFAULT_SEQ_TOKEN + DEFAULT_SEQ_END_TOKEN\n",
    "\n",
    "if SEQUENCE_PLACEHOLDER in instruction:\n",
    "                prompt = instruction.replace(SEQUENCE_PLACEHOLDER, seq_token_se)\n",
    "else:\n",
    "                prompt = seq_token_se + \"\\n\" + instruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pannot.conversation import conv_templates\n",
    "\n",
    "\n",
    "if \"llama\" in model_name.lower():\n",
    "        conv_mode = \"pannot_llama_2\"\n",
    "elif \"mistral\" in model_name.lower():\n",
    "        conv_mode = \"mistral_instruct\"\n",
    "elif \"v1.6-34b\" in model_name.lower():\n",
    "        conv_mode = \"chatml_direct\"\n",
    "elif \"v1\" in model_name.lower():\n",
    "        conv_mode = \"pannot_v1\"\n",
    "else:\n",
    "        conv_mode = \"pannot_v0\"\n",
    "\n",
    "conv = conv_templates[conv_mode].copy()\n",
    "conv.append_message(conv.roles[0], prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "full_prompt = conv.get_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0367e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_prompt = tokenizer_protein_token(full_prompt, tokenizer, return_tensors=\"pt\").unsqueeze(0)\n",
    "input_ids = tokenized_prompt.to(model.device)\n",
    "attention_mask = torch.ones_like(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babab0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if hasattr(model, 'get_seq_tower'):\n",
    "        tokenized_seq = model.get_seq_tower().tokenize(sequence)\n",
    "        seq_input_id = tokenized_seq[\"input_ids\"].squeeze(0).to(model.device)\n",
    "        seq_attention_mask = tokenized_seq[\"attention_mask\"].squeeze(0).to(model.device)\n",
    "        seqs = [seq_input_id]\n",
    "        seq_attention_masks = [seq_attention_mask]\n",
    "else:\n",
    "        seqs = None\n",
    "        seq_attention_masks = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ccf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pannot.eval.eval_opi.eval_opi_dataset_pannot import KeywordsStoppingCriteria\n",
    "\n",
    "\n",
    "stopping_criteria = [KeywordsStoppingCriteria(stopping_keywords, tokenizer, input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e68848",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.5\n",
    "top_p = 0.75\n",
    "num_beams = 1\n",
    "max_new_tokens = 256\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "                    inputs=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    seqs=seqs,\n",
    "                    seq_attention_mask=seq_attention_masks,\n",
    "                    do_sample=temperature > 0,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    num_beams=num_beams,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    use_cache=True,\n",
    "                    eos_token_id=eos_token_ids,\n",
    "                    pad_token_id=pad_token_id,\n",
    "                    stopping_criteria=stopping_criteria\n",
    "                )\n",
    "pred = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea7911-c5b8-41ba-bfe7-0adfa2f9a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load base LLM\n",
    "# model = PannotLlamaForCausalLM.from_pretrained(\n",
    "#     # \"/scratch/user/yining_yang/TAMU/PhD/Pannot/local_pretrained_llm/Meta-Llama-3.1-8B-Instruct\",\n",
    "#     \"/scratch/user/yining_yang/TAMU/PhD/Pannot/checkpoints/pannot-Meta-Llama-3.1-8B-Instruct-pretrain-v00\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",  # automatic single-device or CPU+GPU placement\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Load adapter weights\n",
    "# adapter_weights = torch.load(\"TAMU/PhD/Pannot/checkpoints/pannot-Meta-Llama-3.1-8B-Instruct-pretrain-v00/mm_projector.bin\", map_location=\"cpu\")\n",
    "# model.load_state_dict(adapter_weights, strict=False)\n",
    "\n",
    "# # Tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/scratch/user/yining_yang/TAMU/PhD/Pannot/local_pretrained_llm/Meta-Llama-3.1-8B-Instruct\")\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"Describe the function of this protein.\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "print(\"Has sequence encoder:\", hasattr(model, \"seq_tower\"))\n",
    "print(\"Has structure encoder:\", hasattr(model, \"struc_tower\"))\n",
    "\n",
    "# Optional: check inner modules\n",
    "print(\"Seq tower type:\", type(model.seq_tower))\n",
    "print(\"Struc tower type:\", type(model.struc_tower))\n",
    "\n",
    "\n",
    "# Run generation\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esm3env)",
   "language": "python",
   "name": "esm3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
