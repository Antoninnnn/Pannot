{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fbdec4",
   "metadata": {},
   "source": [
    "## Encoder Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe6b47",
   "metadata": {},
   "source": [
    "In this notebook, we will look into the different encoders (for sequence and structure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534359bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee175ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMSeqEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'facebook/esm2_t6_8M_UR50D',\n",
    "        args=None,\n",
    "        delay_load: bool = False,\n",
    "        no_pooling: bool = False,   # NEW: return full per-residue embeddings?\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.select_layer = getattr(args, 'protein_select_layer', -1)\n",
    "        self.pooling = getattr(args, 'protein_pooling', 'cls')  # 'cls' or 'mean'\n",
    "        self.no_pooling = no_pooling  # NEW flag\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "\n",
    "    def load_model(self, device_map=None):\n",
    "        if self.is_loaded:\n",
    "            print(f'{self.model_name} is already loaded. Skipping load.')\n",
    "            return\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        # Freeze encoder weights by default\n",
    "        self.encoder.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def tokenize(self, sequences):\n",
    "        return self.tokenizer(\n",
    "            sequences,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, sequences):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        # Tokenize & move to model device\n",
    "        inputs = self.tokenize(sequences)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Run through ESM, grab hidden states\n",
    "        outputs = self.encoder(**inputs)\n",
    "        # hidden_states is a tuple: (layer0, layer1, ..., layerN)\n",
    "        hidden_states = outputs.hidden_states[self.select_layer]  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        if self.no_pooling:\n",
    "            # Return full sequence embeddings\n",
    "            return hidden_states\n",
    "\n",
    "        # Otherwise pool to single vector per sequence\n",
    "        if self.pooling == 'cls':\n",
    "            # CLS token is at position 0\n",
    "            features = hidden_states[:, 0, :]\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand_as(hidden_states)\n",
    "            sum_emb = torch.sum(hidden_states * mask, dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            features = sum_emb / counts\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling type: {self.pooling}\")\n",
    "\n",
    "        return features\n",
    "    \n",
    "    # Load sequence from FASTA\n",
    "    def load_fasta_sequence(self, fasta_path):\n",
    "        record = next(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "        return str(record.seq)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        if not self.is_loaded:\n",
    "            # If not loaded, infer from config (usually fp32)\n",
    "            return torch.get_default_dtype()\n",
    "        return self.encoder.dtype\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        if not self.is_loaded:\n",
    "            return torch.device('cpu')\n",
    "        # encoder.device may be a map for multi-GPU; pick first\n",
    "        dev = next(self.encoder.parameters()).device\n",
    "        return dev\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.encoder.config\n",
    "        return PretrainedConfig.from_pretrained(self.model_name)\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        \"\"\"\n",
    "        Returns a zero tensor matching the shape of the output:\n",
    "        - (1, seq_len, hidden_size) if sequence_output, else (1, hidden_size)\n",
    "        Note: seq_len = 1 for dummy by default.\n",
    "        \"\"\"\n",
    "        if self.no_pooling:\n",
    "            # dummy single residue embedding\n",
    "            return torch.zeros(1, 1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010de6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/esm2_t6_8M_UR50D is already loaded. Skipping load.\n",
      "Output shape: torch.Size([1, 78, 320])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = ESMSeqEncoder(\n",
    "    model_name='facebook/esm2_t6_8M_UR50D',\n",
    "    no_pooling=True  # Set to True if you want per-residue embeddings\n",
    ")\n",
    "encoder.load_model()\n",
    "\n",
    "# Load test sequence\n",
    "sequence = encoder.load_fasta_sequence(\"../asset/demo_seq_str/pdb_1ubq/rcsb_pdb_1UBQ.fasta\")\n",
    "\n",
    "# Run through encoder\n",
    "with torch.no_grad():\n",
    "    output = encoder([sequence])  # Note: input is a list of sequences\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # (1, hidden_size) or (1, seq_len, hidden_size) if no_pooling=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0ba9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1612,  0.9960, -0.2067,  ...,  1.1084, -0.2607, -0.5740],\n",
       "         [-0.0533,  0.7383, -0.3275,  ...,  0.6771, -0.3348,  0.0325],\n",
       "         [ 0.0191,  0.1796, -0.0121,  ...,  0.2409,  0.1158,  0.0507],\n",
       "         ...,\n",
       "         [ 0.8836, -0.0777, -0.1994,  ...,  0.2270,  0.3404,  0.0509],\n",
       "         [ 0.3051, -0.4682,  0.1557,  ..., -0.3124,  0.4537, -0.8090],\n",
       "         [ 0.1718,  0.5675,  0.0740,  ...,  0.2063, -0.1933, -0.4174]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61c59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import esm\n",
    "from esm.inverse_folding.util import extract_coords_from_structure, get_encoder_output\n",
    "\n",
    "from typing import Optional\n",
    "from biotite.structure import AtomArray\n",
    "from biotite.structure.io.pdbx import CIFFile, get_structure as get_structure_cif\n",
    "from biotite.structure.io.pdb import PDBFile, get_structure as get_structure_pdb\n",
    "import biotite.structure as struc  # for filter_peptide_backbone\n",
    "\n",
    "def load_structure(\n",
    "    file_path: str,\n",
    "    chain: Optional[str] = None,\n",
    "    model: int = 1\n",
    ") -> AtomArray:\n",
    "    \"\"\"\n",
    "    Load a protein structure from .cif/.mmcif or .pdb, select one model & chain,\n",
    "    then filter to peptide backbone atoms only.\n",
    "    \"\"\"\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    # Read & convert to AtomArray\n",
    "    if ext in (\"cif\", \"mmcif\"):\n",
    "        cif    = CIFFile.read(file_path)\n",
    "        struct = get_structure_cif(cif, model=model)\n",
    "    elif ext == \"pdb\":\n",
    "        pdb    = PDBFile.read(file_path)\n",
    "        struct = get_structure_pdb(pdb, model=model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension '.{ext}'\")\n",
    "\n",
    "    # Optional chain selection\n",
    "    if chain is not None:\n",
    "        struct = struct[struct.chain_id == chain]\n",
    "\n",
    "    # **Filter to peptide backbone (drops waters, side-chains, non-standard residues)**\n",
    "    backbone_mask = struc.filter_peptide_backbone(struct)\n",
    "    struct = struct[backbone_mask]\n",
    "\n",
    "    return struct\n",
    "\n",
    "class ESMIFEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"esm_if1_gvp4_t16_142M_UR50\",\n",
    "        args=None,\n",
    "        delay_load: bool = False,\n",
    "        no_pooling: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.no_pooling = no_pooling\n",
    "        self.is_loaded = False\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.is_loaded:\n",
    "            print(f\"{self.model_name} already loaded. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # Load the inverse-folding model and its alphabet\n",
    "        model, alphabet = getattr(esm.pretrained, self.model_name)()\n",
    "        model = model.eval().requires_grad_(False)\n",
    "        self.model = model\n",
    "        self.alphabet = alphabet\n",
    "        self.is_loaded = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, structure_path: str, chain: str = None):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        # 1) Load and filter backbone atoms / select chain\n",
    "        structure = load_structure(structure_path, chain)\n",
    "\n",
    "        # 2) Extract (L × 3 × 3) coords tensor + sequence string\n",
    "        coords, seq = extract_coords_from_structure(structure)\n",
    "\n",
    "        # 3) Convert coords to torch tensor\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "        # 4) Run the inverse-folding model\n",
    "        encoder_out = get_encoder_output(self.model, self.alphabet, coords_tensor)\n",
    "        # embeddings = encoder_out[\"representations\"]  # (L, hidden_size)\n",
    "        embeddings = encoder_out\n",
    "        \n",
    "\n",
    "        if self.no_pooling:\n",
    "            # Return per-residue (1, L, hidden_size)\n",
    "            return embeddings.unsqueeze(0)\n",
    "        else:\n",
    "            # Mean pool over L residues → (1, hidden_size)\n",
    "            return embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        if not self.is_loaded:\n",
    "            return torch.device(\"cpu\")\n",
    "        return next(self.model.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        if not self.is_loaded:\n",
    "            return torch.get_default_dtype()\n",
    "        return next(self.model.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        return self.model.embed_dim\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        \"\"\"\n",
    "        - If no_pooling=True: returns (1,1,hidden_size)\n",
    "        - Else: (1,hidden_size)\n",
    "        \"\"\"\n",
    "        if self.no_pooling:\n",
    "            return torch.zeros(1, 1, self.hidden_size,\n",
    "                               device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            return torch.zeros(1, self.hidden_size,\n",
    "                               device=self.device, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30a0a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n",
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/inverse_folding/util.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.inf)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder\n",
    "encoder = ESMIFEncoder(no_pooling = True)\n",
    "\n",
    "# Specify the path to your PDB or mmCIF file\n",
    "structure_path = \"../asset/demo_seq_str/pdb_1ubq/1ubq.cif\"\n",
    "\n",
    "# Optionally, specify the chain of interest\n",
    "chain = \"A\"\n",
    "\n",
    "# Get the protein embeddings\n",
    "embeddings = encoder(structure_path, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de297b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 76, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb272e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe0436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a22eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import esm\n",
    "# from esm.inverse_folding.util import load_structure, extract_coords_from_structure, get_encoder_output\n",
    "\n",
    "# class ESMIFEncoder(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_name: str = \"esm_if1_gvp4_t16_142M_UR50\",\n",
    "#         args=None,\n",
    "#         delay_load: bool = False,\n",
    "#         no_pooling: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.model_name = model_name\n",
    "#         self.no_pooling = no_pooling\n",
    "#         self.is_loaded = False\n",
    "\n",
    "#         if not delay_load:\n",
    "#             self.load_model()\n",
    "\n",
    "#     def load_model(self):\n",
    "#         if self.is_loaded:\n",
    "#             print(f\"{self.model_name} already loaded. Skipping.\")\n",
    "#             return\n",
    "\n",
    "#         # Load the inverse-folding model and its alphabet\n",
    "#         model, alphabet = getattr(esm.pretrained, self.model_name)()\n",
    "#         model = model.eval().requires_grad_(False)\n",
    "#         self.model = model\n",
    "#         self.alphabet = alphabet\n",
    "#         self.is_loaded = True\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, structure_path: str, chain: str = None):\n",
    "#         if not self.is_loaded:\n",
    "#             self.load_model()\n",
    "\n",
    "#         # 1) Load and filter backbone atoms / select chain\n",
    "#         structure = load_structure(structure_path, chain)\n",
    "\n",
    "#         # 2) Extract (L × 3 × 3) coords tensor + sequence string\n",
    "#         coords, seq = extract_coords_from_structure(structure)\n",
    "\n",
    "#         # 3) Convert coords to torch tensor\n",
    "#         coords_tensor = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "#         # 4) Run the inverse-folding model\n",
    "#         encoder_out = get_encoder_output(self.model, self.alphabet, coords_tensor)\n",
    "#         # embeddings = encoder_out[\"representations\"]  # (L, hidden_size)\n",
    "#         embeddings = encoder_out\n",
    "        \n",
    "\n",
    "#         if self.no_pooling:\n",
    "#             # Return per-residue (1, L, hidden_size)\n",
    "#             return embeddings.unsqueeze(0)\n",
    "#         else:\n",
    "#             # Mean pool over L residues → (1, hidden_size)\n",
    "#             return embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         if not self.is_loaded:\n",
    "#             return torch.device(\"cpu\")\n",
    "#         return next(self.model.parameters()).device\n",
    "\n",
    "#     @property\n",
    "#     def dtype(self):\n",
    "#         if not self.is_loaded:\n",
    "#             return torch.get_default_dtype()\n",
    "#         return next(self.model.parameters()).dtype\n",
    "\n",
    "#     @property\n",
    "#     def hidden_size(self):\n",
    "#         if not self.is_loaded:\n",
    "#             self.load_model()\n",
    "#         return self.model.embed_dim\n",
    "\n",
    "#     @property\n",
    "#     def dummy_feature(self):\n",
    "#         \"\"\"\n",
    "#         - If no_pooling=True: returns (1,1,hidden_size)\n",
    "#         - Else: (1,hidden_size)\n",
    "#         \"\"\"\n",
    "#         if self.no_pooling:\n",
    "#             return torch.zeros(1, 1, self.hidden_size,\n",
    "#                                device=self.device, dtype=self.dtype)\n",
    "#         else:\n",
    "#             return torch.zeros(1, self.hidden_size,\n",
    "#                                device=self.device, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9acce39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mesm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_folding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoder_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphabet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/inverse_folding/util.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "esm.inverse_folding.util.get_encoder_output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from typing import Optional\n",
    "# from biotite.structure import AtomArray\n",
    "# from biotite.structure.io.pdbx import CIFFile, get_structure as get_structure_cif\n",
    "# from biotite.structure.io.pdb import PDBFile, get_structure as get_structure_pdb\n",
    "# import biotite.structure as struc  # for filter_peptide_backbone\n",
    "\n",
    "# atom_names = [\n",
    "#     \"N\", \"CA\", \"C\", \"O\", \"CB\",\n",
    "#     \"CG\", \"CG1\", \"CG2\", \"CD\", \"CD1\", \"CD2\",\n",
    "#     \"CE\", \"CE1\", \"CE2\", \"CE3\", \"CZ\", \"CZ2\", \"CZ3\", \"CH2\",\n",
    "#     \"ND1\", \"ND2\", \"NE\", \"NE1\", \"NE2\", \"NH1\", \"NH2\",\n",
    "#     \"NZ\", \"OD1\", \"OD2\", \"OE1\", \"OE2\", \"OG\", \"OG1\", \"OH\",\n",
    "#     \"SD\", \"SG\"\n",
    "# ]\n",
    "\n",
    "# def load_structure(\n",
    "#     file_path: str,\n",
    "#     chain: Optional[str] = None,\n",
    "#     model: int = 1\n",
    "# ) -> AtomArray:\n",
    "#     \"\"\"\n",
    "#     Load a protein structure from .cif/.mmcif or .pdb, select one model & chain,\n",
    "#     then filter to peptide backbone atoms only.\n",
    "#     \"\"\"\n",
    "#     ext = file_path.split('.')[-1].lower()\n",
    "#     if ext == \"pkl\":\n",
    "#     with open(file_path, \"rb\") as f:\n",
    "#         data = pickle.load(f)\n",
    "\n",
    "#     coords_all_atoms = data[\"atom_positions\"]        # (L, 37, 3)\n",
    "#     atom_mask = data[\"atom_mask\"]                    # (L, 37)\n",
    "#     aatype = data[\"aatype\"]                          # (L,)\n",
    "#     res_idx = data[\"residue_index\"]                  # (L,)\n",
    "#     chain_idx = data[\"chain_index\"]                  # (L,)\n",
    "#     modeled_idx = data.get(\"modeled_idx\", np.arange(len(aatype)))\n",
    "\n",
    "#     # Fixed order of 37 standard atom names\n",
    "#     fixed_atom_names = [\n",
    "#         \"N\", \"CA\", \"C\", \"O\", \"CB\",\n",
    "#         \"CG\", \"CG1\", \"CG2\", \"CD\", \"CD1\", \"CD2\",\n",
    "#         \"CE\", \"CE1\", \"CE2\", \"CE3\", \"CZ\", \"CZ2\", \"CZ3\", \"CH2\",\n",
    "#         \"ND1\", \"ND2\", \"NE\", \"NE1\", \"NE2\", \"NH1\", \"NH2\",\n",
    "#         \"NZ\", \"OD1\", \"OD2\", \"OE1\", \"OE2\", \"OG\", \"OG1\", \"OH\",\n",
    "#         \"SD\", \"SG\"\n",
    "#     ]\n",
    "\n",
    "#     # Collect atom-level data\n",
    "#     atom_name_list = []\n",
    "#     coord_list = []\n",
    "#     res_id_list = []\n",
    "#     chain_id_list = []\n",
    "\n",
    "#     for i in range(coords_all_atoms.shape[0]):\n",
    "#         res_atoms = coords_all_atoms[i]          # (37, 3)\n",
    "#         res_mask = atom_mask[i] > 0.0            # (37,)\n",
    "\n",
    "#         for j in range(37):\n",
    "#             if not res_mask[j]:\n",
    "#                 continue\n",
    "#             atom_name_list.append(fixed_atom_names[j])\n",
    "#             coord_list.append(res_atoms[j])\n",
    "#             res_id_list.append(res_idx[i])\n",
    "#             chain_id_list.append(chr(65 + chain_idx[i]))  # assumes A-Z chains\n",
    "\n",
    "#     # Build AtomArray\n",
    "#     coords = np.array(coord_list)\n",
    "#     atom_names = np.array(atom_name_list)\n",
    "#     res_ids = np.array(res_id_list)\n",
    "#     chain_ids = np.array(chain_id_list)\n",
    "\n",
    "#     atoms = AtomArray(len(coords))\n",
    "#     atoms.coord = coords\n",
    "#     atoms.atom_name = atom_names\n",
    "#     atoms.res_id = res_ids\n",
    "#     atoms.chain_id = chain_ids\n",
    "#     atoms.element = guess_element(atom_names)\n",
    "\n",
    "#     # Optional chain filtering\n",
    "#     if chain is not None:\n",
    "#         atoms = atoms[atoms.chain_id == chain]\n",
    "\n",
    "#     # Keep only backbone atoms (N, CA, C)\n",
    "#     backbone_mask = filter_peptide_backbone(atoms)\n",
    "#     return atoms[backbone_mask]\n",
    "#     elif ext in (\"cif\", \"mmcif\"):\n",
    "#         cif = CIFFile.read(file_path)\n",
    "#         struct = get_structure_cif(cif, model=model)\n",
    "#     elif ext == \"pdb\":\n",
    "#         pdb    = PDBFile.read(file_path)\n",
    "#         struct = get_structure_pdb(pdb, model=model)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unsupported extension '.{ext}'\")\n",
    "\n",
    "#     # Optional chain selection\n",
    "#     if chain is not None:\n",
    "#         struct = struct[struct.chain_id == chain]\n",
    "\n",
    "#     # **Filter to peptide backbone (drops waters, side-chains, non-standard residues)**\n",
    "#     backbone_mask = struc.filter_peptide_backbone(struct)\n",
    "#     struct = struct[backbone_mask]\n",
    "\n",
    "#     return struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5dd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ce219f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import esm\n",
    "# from esm.data import Alphabet\n",
    "# from esm.inverse_folding.util import extract_coords_from_structure\n",
    "# # from esm.inverse_folding.util import extract_coords_from_complex\n",
    "\n",
    "# from biotite.structure import filter_amino_acids\n",
    "\n",
    "# # patch esm so hub loader finds Alphabet\n",
    "# esm.Alphabet = Alphabet\n",
    "\n",
    "# class ESMIFEncoder(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_name: str = \"esm_if1_gvp4_t16_142M_UR50\",\n",
    "#         args=None,\n",
    "#         delay_load: bool = False,filter_peptide_backbone\n",
    "#         no_pooling: bool = False,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.model_name = model_name\n",
    "#         self.no_pooling = no_pooling\n",
    "#         self.is_loaded = False\n",
    "#         if not delay_load:\n",
    "#             self._load_model()\n",
    "\n",
    "#     def _load_model(self):\n",
    "#         if self.is_loaded:\n",
    "#             return\n",
    "#         model, alphabet = torch.hub.load(\n",
    "#             \"facebookresearch/esm:main\",\n",
    "#             self.model_name\n",
    "#         )\n",
    "#         self.model = model.eval().requires_grad_(False)\n",
    "#         self.alphabet = alphabet\n",
    "#         self.is_loaded = True\n",
    "\n",
    "#     # @torch.no_grad()\n",
    "#     # def forward(self, structure: AtomArray):\n",
    "#     #     \"\"\"\n",
    "#     #     Expects a loaded AtomArray (via load_structure), not a path.\n",
    "#     #     \"\"\"\n",
    "#     #     if not self.is_loaded:\n",
    "#     #         self._load_model()\n",
    "\n",
    "#     #     # Extract coordinates and sequence for the complex structure\n",
    "#     #     coords, native_seqs = extract_coords_from_structure(structure)\n",
    "\n",
    "#     #     # Convert coords to a tensor\n",
    "#     #     coords = torch.tensor(coords, dtype=torch.float32)\n",
    "#     #     coords = coords.to(self.device)\n",
    "\n",
    "#     #     # Directly call the model without passing the additional arguments\n",
    "#     #     out = self.model({\"coords\": coords})\n",
    "\n",
    "#     #     # Extract embeddings\n",
    "#     #     emb = out[\"representations\"]\n",
    "\n",
    "#     #     # Return either pooled or per-residue embeddings\n",
    "#     #     if self.no_pooling:\n",
    "#     #         return emb.unsqueeze(0)  # Add batch dimension\n",
    "#     #     return emb.mean(dim=0, keepdim=True)  # Pool embeddings\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def forward(self, structure: AtomArray):\n",
    "#         \"\"\"\n",
    "#         Expects a loaded AtomArray (via load_structure), not a path.\n",
    "#         \"\"\"\n",
    "#         if not self.is_loaded:\n",
    "#             self._load_model()\n",
    "\n",
    "#         # Extract coordinates and sequence from the structure\n",
    "#         coords, seq = extract_coords_from_structure(structure)\n",
    "\n",
    "#         # Convert coords to a tensor\n",
    "#         coords = torch.tensor(coords, dtype=torch.float32)\n",
    "#         coords = coords.to(self.device)\n",
    "\n",
    "#         # Create default \"empty\" values for the missing arguments\n",
    "#         padding_mask = torch.zeros((coords.shape[0],), dtype=torch.bool, device=self.device)  # Dummy padding mask\n",
    "#         confidence = torch.ones((coords.shape[0],), dtype=torch.float32, device=self.device)  # Dummy confidence\n",
    "#         prev_output_tokens = torch.zeros((coords.shape[0],), dtype=torch.long, device=self.device)  # Dummy previous tokens\n",
    "\n",
    "#         # Call the model with the required dummy values\n",
    "#         out = self.model({\n",
    "#             \"coords\": coords,\n",
    "#             \"padding_mask\": padding_mask,\n",
    "#             \"confidence\": confidence,\n",
    "#             \"prev_output_tokens\": prev_output_tokens\n",
    "#         })\n",
    "\n",
    "#         # Extract embeddings\n",
    "#         emb = out[\"representations\"]\n",
    "\n",
    "#         # Return either pooled or per-residue embeddings\n",
    "#         if self.no_pooling:\n",
    "#             return emb.unsqueeze(0)  # Add batch dimension\n",
    "#         return emb.mean(dim=0, keepdim=True)  # Pool embeddings\n",
    "\n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         return (next(self.model.parameters()).device\n",
    "#                 if self.is_loaded else torch.device(\"cpu\"))\n",
    "\n",
    "#     @property\n",
    "#     def dtype(self):\n",
    "#         return (next(self.model.parameters()).dtype\n",
    "#                 if self.is_loaded else torch.get_default_dtype())\n",
    "\n",
    "#     @property\n",
    "#     def hidden_size(self):\n",
    "#         if not self.is_loaded:\n",
    "#             self._load_model()\n",
    "#         return self.model.embed_dim\n",
    "\n",
    "#     @property\n",
    "#     def dummy_feature(self):\n",
    "#         if self.no_pooling:\n",
    "#             return torch.zeros(1, 1, self.hidden_size,\n",
    "#                                device=self.device, dtype=self.dtype)\n",
    "#         return torch.zeros(1, self.hidden_size,\n",
    "#                            device=self.device, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5fc4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from biotite.structure import filter_peptide_backbone\n",
    "import esm.inverse_folding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc9e5fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from esm.inverse_folding.model import InverseFoldingModel\n",
    "# from esm.inverse_folding.util  import load_coords, extract_coords_from_pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15438b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'biotite.structure.io.pdbx' has no attribute 'PDBxFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get the protein embeddings\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m encoder(structure_path, chain)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mESMIFEncoder.forward\u001b[0;34m(self, structure_path, chain)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 1) Load and filter backbone atoms / select chain\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m structure \u001b[38;5;241m=\u001b[39m load_structure(structure_path, chain)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 2) Extract (L × 3 × 3) coords tensor + sequence string\u001b[39;00m\n\u001b[1;32m     43\u001b[0m coords, seq \u001b[38;5;241m=\u001b[39m extract_coords_from_structure(structure)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/inverse_folding/util.py:37\u001b[0m, in \u001b[0;36mload_structure\u001b[0;34m(fpath, chain)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcif\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fpath) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m---> 37\u001b[0m         pdbxf \u001b[38;5;241m=\u001b[39m pdbx\u001b[38;5;241m.\u001b[39mPDBxFile\u001b[38;5;241m.\u001b[39mread(fin)\n\u001b[1;32m     38\u001b[0m     structure \u001b[38;5;241m=\u001b[39m pdbx\u001b[38;5;241m.\u001b[39mget_structure(pdbxf, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'biotite.structure.io.pdbx' has no attribute 'PDBxFile'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c45df4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/inverse_folding/util.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.inf)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# structure = load_structure(pdb_file, chain=\"A\", model=1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming 'structure' is your AtomArray or AtomArrayStack\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# structure = filter_amino_acids(structure)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 3.2) Initialize and run the encoder\u001b[39;00m\n\u001b[1;32m     10\u001b[0m encoder \u001b[38;5;241m=\u001b[39m ESMIFEncoder(no_pooling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m encoder(pdb_file)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 3.3) Inspect output\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[48], line 50\u001b[0m, in \u001b[0;36mESMIFEncoder.forward\u001b[0;34m(self, structure_path, chain)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# 4) Run the inverse-folding model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m get_encoder_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet, coords_tensor)\n\u001b[0;32m---> 50\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m encoder_out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# (L, hidden_size)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_pooling:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# Return per-residue (1, L, hidden_size)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 3.1) Load your structure from CIF\n",
    "# pdb_file = \"../asset/demo_seq_str/pdb_1ubq/1ubq.cif\"\n",
    "\n",
    "\n",
    "# # structure = load_structure(pdb_file, chain=\"A\", model=1)\n",
    "\n",
    "# # Assuming 'structure' is your AtomArray or AtomArrayStack\n",
    "# # structure = filter_amino_acids(structure)\n",
    "# # 3.2) Initialize and run the encoder\n",
    "# encoder = ESMIFEncoder(no_pooling=False)\n",
    "# embeddings = encoder(pdb_file)\n",
    "\n",
    "# # 3.3) Inspect output\n",
    "# print(\"Embeddings shape:\", embeddings.shape)\n",
    "# # → (1, L, hidden_size), where L = number of residues in chain A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62cd03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'axial_attention', 'constants', 'data', 'inverse_folding', 'modules', 'multihead_attention', 'rotary_embedding']\n"
     ]
    }
   ],
   "source": [
    "print(dir(esm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64054817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62753d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['atom_positions', 'aatype', 'atom_mask', 'residue_index', 'chain_index', 'b_factors', 'bb_mask', 'bb_positions', 'modeled_idx'])\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# Path to the .pkl file\n",
    "file_path = '../asset/demo_seq_str/pdb_1ubq/1ubq.pkl'\n",
    "\n",
    "# Open the file and load the content\n",
    "with open(file_path, 'rb') as file:\n",
    "    pkl_data = pickle.load(file)\n",
    "\n",
    "# Now, `data` contains the object that was saved in the .pkl file\n",
    "print(pkl_data.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d7c35cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-3.10289063e+00, -4.57898263e+00, -1.29023926e+01],\n",
       "        [-4.17689003e+00, -3.59598283e+00, -1.26743927e+01],\n",
       "        [-3.52989067e+00, -2.36998299e+00, -1.19853928e+01],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]],\n",
       "\n",
       "       [[-4.10789169e+00, -1.23898247e+00, -1.22583928e+01],\n",
       "        [-3.59289040e+00,  1.20169763e-02, -1.16183927e+01],\n",
       "        [-4.34289040e+00,  2.44017327e-01, -1.03143925e+01],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]],\n",
       "\n",
       "       [[-3.59388985e+00,  6.47017205e-01, -9.29939266e+00],\n",
       "        [-4.20789017e+00,  1.04901763e+00, -8.01939245e+00],\n",
       "        [-3.56089081e+00,  2.41901656e+00, -7.65439268e+00],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]],\n",
       "\n",
       "       [[-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]],\n",
       "\n",
       "       [[-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        ...,\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00],\n",
       "        [-0.00000000e+00, -0.00000000e+00, -0.00000000e+00]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_data[\"atom_positions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b81a953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59ddbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Extract relevant data from the .pkl file (assuming these are already NumPy arrays)\n",
    "# atom_positions = pkl_data['atom_positions']  # Expected shape: (n_residue, 37, 3)\n",
    "# aatype = pkl_data['aatype']  # Amino acid types as integers\n",
    "# atom_mask = pkl_data['atom_mask']  # Mask for atoms\n",
    "# bb_mask = pkl_data['bb_mask']  # Backbone mask\n",
    "# bb_positions = pkl_data['bb_positions']  # Backbone positions\n",
    "# residue_index = pkl_data['residue_index']  # Residue indices\n",
    "# chain_index = pkl_data['chain_index']  # Chain indices\n",
    "# modeled_idx = pkl_data['modeled_idx']  # Modeled atom indices\n",
    "\n",
    "# # Reshape atom_positions to be 2D: (num_atoms, 3)\n",
    "# num_residues = atom_positions.shape[0]\n",
    "# num_atoms_per_residue = atom_positions.shape[1]  # 37\n",
    "# reshaped_atom_positions = atom_positions.reshape(-1, 3)  # Flatten the residues into a single array of atoms\n",
    "\n",
    "# # Initialize AtomArray with the correct length (total number of atoms)\n",
    "# num_atoms = reshaped_atom_positions.shape[0]\n",
    "# structure = AtomArray(num_atoms)\n",
    "\n",
    "# # Directly assign the reshaped NumPy arrays to the AtomArray attributes\n",
    "# structure.coord = reshaped_atom_positions  # Now it's a 2D array (num_atoms, 3)\n",
    "# structure.aatype = np.repeat(aatype, num_atoms_per_residue)  # Repeat amino acid types for each atom\n",
    "# structure.atom_mask = np.repeat(atom_mask, num_atoms_per_residue)  # Repeat atom mask\n",
    "# structure.bb_mask = np.repeat(bb_mask, num_atoms_per_residue)  # Repeat backbone mask\n",
    "# structure.bb_positions = np.repeat(bb_positions, num_atoms_per_residue, axis=0)  # Repeat backbone positions\n",
    "# structure.residue_index = np.repeat(residue_index, num_atoms_per_residue)  # Repeat residue index\n",
    "# structure.chain_index = np.repeat(chain_index, num_atoms_per_residue)  # Repeat chain index\n",
    "# structure.modeled_idx = np.repeat(modeled_idx, num_atoms_per_residue)  # Repeat modeled atom indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51a2739f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Forward pass through the model (using the structure)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m encoder(file_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, embeddings\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m, in \u001b[0;36mESMIFEncoder.forward\u001b[0;34m(self, structure_path, chain)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 1) Load and filter backbone atoms / select chain\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m structure \u001b[38;5;241m=\u001b[39m load_structure(structure_path, chain)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# 2) Extract (L × 3 × 3) coords tensor + sequence string\u001b[39;00m\n\u001b[1;32m     43\u001b[0m coords, seq \u001b[38;5;241m=\u001b[39m extract_coords_from_structure(structure)\n",
      "Cell \u001b[0;32mIn[27], line 61\u001b[0m, in \u001b[0;36mload_structure\u001b[0;34m(file_path, chain, model)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res_mask[j]:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m atom_name_list\u001b[38;5;241m.\u001b[39mappend(fixed_atom_names[j])\n\u001b[1;32m     62\u001b[0m coord_list\u001b[38;5;241m.\u001b[39mappend(res_atoms[j])\n\u001b[1;32m     63\u001b[0m res_id_list\u001b[38;5;241m.\u001b[39mappend(res_idx[i])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Now, structure is an instance of AtomArray and can be passed to the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the encoder\n",
    "encoder = ESMIFEncoder(no_pooling=True)\n",
    "\n",
    "# Forward pass through the model (using the structure)\n",
    "with torch.no_grad():\n",
    "    embeddings = encoder(file_path)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf881d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
