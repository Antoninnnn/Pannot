{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2fbdec4",
   "metadata": {},
   "source": [
    "## Encoder Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe6b47",
   "metadata": {},
   "source": [
    "In this notebook, we will look into the different encoders (for sequence and structure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534359bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee175ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMSeqEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'facebook/esm2_t6_8M_UR50D',\n",
    "        args=None,\n",
    "        delay_load: bool = False,\n",
    "        no_pooling: bool = True,   # NEW: return full per-residue embeddings?\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.is_loaded = False\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.select_layer = getattr(args, 'protein_select_layer', -1)\n",
    "        self.pooling = getattr(args, 'protein_pooling', 'cls')  # 'cls' or 'mean'\n",
    "        self.no_pooling = no_pooling  # NEW flag\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "\n",
    "    def load_model(self, device_map=None):\n",
    "        if self.is_loaded:\n",
    "            print(f'{self.model_name} is already loaded. Skipping load.')\n",
    "            return\n",
    "\n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            device_map=device_map,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        # Freeze encoder weights by default\n",
    "        self.encoder.requires_grad_(False)\n",
    "\n",
    "        self.is_loaded = True\n",
    "\n",
    "    def tokenize(self, sequences):\n",
    "        return self.tokenizer(\n",
    "            sequences,\n",
    "            return_tensors='pt',\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, sequences):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        # Tokenize & move to model device\n",
    "        inputs = self.tokenize(sequences)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Run through ESM, grab hidden states\n",
    "        outputs = self.encoder(**inputs)\n",
    "        # hidden_states is a tuple: (layer0, layer1, ..., layerN)\n",
    "        hidden_states = outputs.hidden_states[self.select_layer]  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        if self.no_pooling:\n",
    "            # Return full sequence embeddings\n",
    "            return hidden_states\n",
    "\n",
    "        # Otherwise pool to single vector per sequence\n",
    "        if self.pooling == 'cls':\n",
    "            # CLS token is at position 0\n",
    "            features = hidden_states[:, 0, :]\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).expand_as(hidden_states)\n",
    "            sum_emb = torch.sum(hidden_states * mask, dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            features = sum_emb / counts\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling type: {self.pooling}\")\n",
    "\n",
    "        return features\n",
    "    \n",
    "    # Load sequence from FASTA\n",
    "    def load_fasta_sequence(self, fasta_path):\n",
    "        record = next(SeqIO.parse(fasta_path, \"fasta\"))\n",
    "        return str(record.seq)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        if not self.is_loaded:\n",
    "            # If not loaded, infer from config (usually fp32)\n",
    "            return torch.get_default_dtype()\n",
    "        return self.encoder.dtype\n",
    "\n",
    "    # @property\n",
    "    # def device(self):\n",
    "    #     if not self.is_loaded:\n",
    "    #         return torch.device('cpu')\n",
    "    #     # encoder.device may be a map for multi-GPU; pick first\n",
    "    #     dev = next(self.encoder.parameters()).device\n",
    "    #     return dev\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.encoder.device\n",
    "\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.encoder.config\n",
    "        return PretrainedConfig.from_pretrained(self.model_name)\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        \"\"\"\n",
    "        Returns a zero tensor matching the shape of the output:\n",
    "        - (1, seq_len, hidden_size) if sequence_output, else (1, hidden_size)\n",
    "        Note: seq_len = 1 for dummy by default.\n",
    "        \"\"\"\n",
    "        if self.no_pooling:\n",
    "            # dummy single residue embedding\n",
    "            return torch.zeros(1, 1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010de6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook/esm2_t6_8M_UR50D is already loaded. Skipping load.\n",
      "Output shape: torch.Size([1, 78, 320])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = ESMSeqEncoder(\n",
    "    model_name='facebook/esm2_t6_8M_UR50D',\n",
    "    no_pooling=True  # Set to True if you want per-residue embeddings\n",
    ")\n",
    "encoder.load_model()\n",
    "\n",
    "# Load test sequence\n",
    "sequence = encoder.load_fasta_sequence(\"../asset/demo_seq_str/pdb_1ubq/rcsb_pdb_1UBQ.fasta\")\n",
    "\n",
    "# Run through encoder\n",
    "with torch.no_grad():\n",
    "    output = encoder(sequence)  # Note: input is a list of sequences\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # (1, hidden_size) or (1, seq_len, hidden_size) if no_pooling=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b0ba9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1612,  0.9960, -0.2067,  ...,  1.1084, -0.2607, -0.5740],\n",
       "         [-0.0533,  0.7383, -0.3275,  ...,  0.6771, -0.3348,  0.0325],\n",
       "         [ 0.0191,  0.1796, -0.0121,  ...,  0.2409,  0.1158,  0.0507],\n",
       "         ...,\n",
       "         [ 0.8836, -0.0777, -0.1994,  ...,  0.2270,  0.3404,  0.0509],\n",
       "         [ 0.3051, -0.4682,  0.1557,  ..., -0.3124,  0.4537, -0.8090],\n",
       "         [ 0.1718,  0.5675,  0.0740,  ...,  0.2063, -0.1933, -0.4174]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61c59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import esm\n",
    "from esm.inverse_folding.util import extract_coords_from_structure, get_encoder_output\n",
    "\n",
    "from typing import Optional\n",
    "from biotite.structure import AtomArray\n",
    "from biotite.structure.io.pdbx import CIFFile, get_structure as get_structure_cif\n",
    "from biotite.structure.io.pdb import PDBFile, get_structure as get_structure_pdb\n",
    "import biotite.structure as struc  # for filter_peptide_backbone\n",
    "\n",
    "def load_structure(\n",
    "    file_path: str,\n",
    "    chain: Optional[str] = None,\n",
    "    model: int = 1\n",
    ") -> AtomArray:\n",
    "    \"\"\"\n",
    "    Load a protein structure from .cif/.mmcif or .pdb, select one model & chain,\n",
    "    then filter to peptide backbone atoms only.\n",
    "    \"\"\"\n",
    "    ext = file_path.split('.')[-1].lower()\n",
    "    # Read & convert to AtomArray\n",
    "    if ext in (\"cif\", \"mmcif\"):\n",
    "        cif    = CIFFile.read(file_path)\n",
    "        struct = get_structure_cif(cif, model=model)\n",
    "    elif ext == \"pdb\":\n",
    "        pdb    = PDBFile.read(file_path)\n",
    "        struct = get_structure_pdb(pdb, model=model)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported extension '.{ext}'\")\n",
    "\n",
    "    # Optional chain selection\n",
    "    if chain is not None:\n",
    "        struct = struct[struct.chain_id == chain]\n",
    "\n",
    "    # **Filter to peptide backbone (drops waters, side-chains, non-standard residues)**\n",
    "    backbone_mask = struc.filter_peptide_backbone(struct)\n",
    "    struct = struct[backbone_mask]\n",
    "\n",
    "    return struct\n",
    "\n",
    "class ESMIFEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"esm_if1_gvp4_t16_142M_UR50\",\n",
    "        args=None,\n",
    "        delay_load: bool = False,\n",
    "        no_pooling: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.no_pooling = no_pooling\n",
    "        self.is_loaded = False\n",
    "\n",
    "        if not delay_load:\n",
    "            self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        if self.is_loaded:\n",
    "            print(f\"{self.model_name} already loaded. Skipping.\")\n",
    "            return\n",
    "\n",
    "        # Load the inverse-folding model and its alphabet\n",
    "        model, alphabet = getattr(esm.pretrained, self.model_name)()\n",
    "        model = model.eval().requires_grad_(False)\n",
    "        self.model = model\n",
    "        self.alphabet = alphabet\n",
    "        self.is_loaded = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, structure_path: str, chain: str = None):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        # 1) Load and filter backbone atoms / select chain\n",
    "        structure = load_structure(structure_path, chain)\n",
    "\n",
    "        # 2) Extract (L × 3 × 3) coords tensor + sequence string\n",
    "        coords, seq = extract_coords_from_structure(structure)\n",
    "\n",
    "        # 3) Convert coords to torch tensor\n",
    "        coords_tensor = torch.tensor(coords, dtype=torch.float32)\n",
    "\n",
    "        # 4) Run the inverse-folding model\n",
    "        encoder_out = get_encoder_output(self.model, self.alphabet, coords_tensor)\n",
    "        # embeddings = encoder_out[\"representations\"]  # (L, hidden_size)\n",
    "        embeddings = encoder_out\n",
    "        \n",
    "\n",
    "        if self.no_pooling:\n",
    "            # Return per-residue (1, L, hidden_size)\n",
    "            return embeddings.unsqueeze(0)\n",
    "        else:\n",
    "            # Mean pool over L residues → (1, hidden_size)\n",
    "            return embeddings.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # @property\n",
    "    # def device(self):\n",
    "    #     if not self.is_loaded:\n",
    "    #         return torch.device(\"cpu\")\n",
    "    #     return next(self.model.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.model.device\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        if not self.is_loaded:\n",
    "            return torch.get_default_dtype()\n",
    "        return next(self.model.parameters()).dtype\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        return self.model.embed_dim\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        \"\"\"\n",
    "        - If no_pooling=True: returns (1,1,hidden_size)\n",
    "        - Else: (1,hidden_size)\n",
    "        \"\"\"\n",
    "        if self.no_pooling:\n",
    "            return torch.zeros(1, 1, self.hidden_size,\n",
    "                               device=self.device, dtype=self.dtype)\n",
    "        else:\n",
    "            return torch.zeros(1, self.hidden_size,\n",
    "                               device=self.device, dtype=self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30a0a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/pretrained.py:215: UserWarning: Regression weights not found, predicting contacts will not produce correct results.\n",
      "  warnings.warn(\n",
      "/home/yining_yang/anaconda3/envs/torch_env/lib/python3.12/site-packages/esm/inverse_folding/util.py:251: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  F.pad(torch.tensor(cd), (0, 0, 0, 0, 1, 1), value=np.inf)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder\n",
    "encoder = ESMIFEncoder(no_pooling = True)\n",
    "\n",
    "# Specify the path to your PDB or mmCIF file\n",
    "structure_path = \"../asset/demo_seq_str/pdb_1ubq/1ubq.cif\"\n",
    "\n",
    "# Optionally, specify the chain of interest\n",
    "chain = \"A\"\n",
    "\n",
    "# Get the protein embeddings\n",
    "embeddings = encoder(structure_path, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de297b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 76, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d292bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_seq_tower(seq_tower_cfg, **kwargs):\n",
    "    seq_tower = getattr(seq_tower_cfg, 'mm_seq_tower', getattr(seq_tower_cfg, 'seq_tower', None))\n",
    "\n",
    "    if seq_tower == 'ESM':\n",
    "        return ESMSeqEncoder(model_name='facebook/esm2_t33_650M_UR50D', args=seq_tower_cfg, **kwargs)\n",
    "\n",
    "    raise ValueError(f'Unknown sequence encoder: {seq_tower}')\n",
    "\n",
    "\n",
    "def build_struc_tower(struc_tower_cfg, **kwargs):\n",
    "    struc_tower = getattr(struc_tower_cfg, 'mm_struc_tower', getattr(struc_tower_cfg, 'struc_tower', None))\n",
    "    if struc_tower == 'ESMIF':\n",
    "        return ESMIFEncoder(model_name='esm_if1_gvp4_t16_142M_UR50', args=struc_tower_cfg, **kwargs)\n",
    "    # elif struc_tower == 'ESM':\n",
    "    #     return ESMStructEncoder(model_name='facebook/esm2_t33_650M_UR50D', args=struc_tower_cfg, **kwargs)\n",
    "    \n",
    "    raise ValueError(f'Unknown structure encoder: {struc_tower}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb272e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "\n",
    "\n",
    "class IdentityMap(nn.Module):\n",
    "    def __init__(self, attr_config: str):\n",
    "        super().__init__()\n",
    "        self.attr = attr_config\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return {self.attr: 'identity'}\n",
    "\n",
    "\n",
    "class SimpleResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.pre_norm = nn.LayerNorm(channels)\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pre_norm(x)\n",
    "        return x + self.proj(x)\n",
    "\n",
    "\n",
    "def build_seq_projector(config, delay_load=False, **kwargs):\n",
    "    projector_type = getattr(config, 'mm_seq_projector_type', 'linear')\n",
    "\n",
    "    if projector_type == 'linear':\n",
    "        return nn.Linear(config.mm_seq_hidden_size, config.hidden_size)\n",
    "\n",
    "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
    "    if mlp_gelu_match:\n",
    "        mlp_depth = int(mlp_gelu_match.group(1))\n",
    "        modules = [nn.Linear(config.mm_seq_hidden_size, config.hidden_size)]\n",
    "        for _ in range(1, mlp_depth):\n",
    "            modules.append(nn.GELU())\n",
    "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    if projector_type == 'identity':\n",
    "        return IdentityMap('mm_seq_projector_type')\n",
    "\n",
    "    raise ValueError(f'Unknown projector type: {projector_type}')\n",
    "\n",
    "\n",
    "def build_struc_projector(config, delay_load=False, **kwargs):\n",
    "    projector_type = getattr(config, 'mm_struc_projector_type', 'linear')\n",
    "\n",
    "    if projector_type == 'linear':\n",
    "        return nn.Linear(config.mm_struc_hidden_size, config.hidden_size)\n",
    "\n",
    "    mlp_gelu_match = re.match(r'^mlp(\\d+)x_gelu$', projector_type)\n",
    "    if mlp_gelu_match:\n",
    "        mlp_depth = int(mlp_gelu_match.group(1))\n",
    "        modules = [nn.Linear(config.mm_struc_hidden_size, config.hidden_size)]\n",
    "        for _ in range(1, mlp_depth):\n",
    "            modules.append(nn.GELU())\n",
    "            modules.append(nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        return nn.Sequential(*modules)\n",
    "\n",
    "    if projector_type == 'identity':\n",
    "        return IdentityMap('mm_struc_projector_type')\n",
    "\n",
    "    raise ValueError(f'Unknown projector type: {projector_type}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7484e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c150567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73fefd91",
   "metadata": {},
   "source": [
    "## pannot_arch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08fce085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# build_vision_tower\n",
    "\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "PROT_TOKEN_INDEX = -300\n",
    "DEFAULT_PROT_TOKEN = \"<prot>\"\n",
    "DEFAULT_PROT_PATCH_TOKEN = \"<prot_patch>\"\n",
    "DEFAULT_PROT_START_TOKEN = \"<prot_start>\"\n",
    "DEFAULT_PROT_END_TOKEN = \"<prot_end>\"\n",
    "PROT_PLACEHOLDER = \"<prot-placeholder>\"\n",
    "\n",
    "SEQ_TOKEN_INDEX = -330\n",
    "DEFAULT_SEQ_TOKEN = \"<seq>\"\n",
    "DEFAULT_SEQ_PATCH_TOKEN = \"<seq_patch>\"\n",
    "DEFAULT_SEQ_START_TOKEN = \"<seq_start>\"\n",
    "DEFAULT_SEQ_END_TOKEN = \"<seq_end>\"\n",
    "\n",
    "STR_TOKEN_INDEX = -360\n",
    "DEFAULT_STR_TOKEN = \"<str>\"\n",
    "DEFAULT_STR_PATCH_TOKEN = \"<str_patch>\"\n",
    "DEFAULT_STR_START_TOKEN = \"<str_start>\"\n",
    "DEFAULT_STR_END_TOKEN = \"<str_end>\"\n",
    "\n",
    "\n",
    "\n",
    "# from pannot.mm_utils import get_anyres_image_grid_shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6e2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PannotMetaModel:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(PannotMetaModel, self).__init__(config)\n",
    "\n",
    "        if hasattr(config, \"mm_seq_tower\"):\n",
    "            self.seq_tower = build_seq_tower(config, delay_load=True)\n",
    "            self.mm_seq_projector = build_seq_projector(config)\n",
    "\n",
    "        if hasattr(config, \"mm_struc_tower\"):\n",
    "            self.struc_tower = build_struc_tower(config, delay_load=True)\n",
    "            self.mm_struc_projector = build_struc_projector(config)\n",
    "\n",
    "    def get_seq_tower(self):\n",
    "        return getattr(self, 'seq_tower', None)\n",
    "\n",
    "    def get_struc_tower(self):\n",
    "        return getattr(self, 'struc_tower', None)\n",
    "\n",
    "    def initialize_seq_modules(self, model_args, fsdp=None):\n",
    "        self.config.mm_seq_tower = model_args.seq_tower\n",
    "        self.config.mm_seq_select_layer = model_args.mm_seq_select_layer\n",
    "        self.config.mm_seq_select_feature = model_args.mm_seq_select_feature\n",
    "        self.config.use_mm_seq_proj = True\n",
    "\n",
    "        if self.get_seq_tower() is None:\n",
    "            seq_tower = build_seq_tower(model_args)\n",
    "            self.seq_tower = [seq_tower] if fsdp else seq_tower\n",
    "        else:\n",
    "            seq_tower = self.seq_tower[0] if fsdp else self.seq_tower\n",
    "            seq_tower.load_model()\n",
    "\n",
    "        self.config.mm_seq_hidden_size = seq_tower.hidden_size\n",
    "        self.config.mm_seq_projector_type = getattr(model_args, 'mm_seq_projector_type', 'linear')\n",
    "\n",
    "        if getattr(self, 'mm_seq_projector', None) is None:\n",
    "            self.mm_seq_projector = build_seq_projector(self.config)\n",
    "        else:\n",
    "            for p in self.mm_seq_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        if model_args.pretrain_mm_seq_mlp_adapter is not None:\n",
    "            seq_projector_weights = torch.load(model_args.pretrain_mm_seq_mlp_adapter, map_location='cpu')\n",
    "            self.mm_seq_projector.load_state_dict(\n",
    "                {k.split('mm_seq_projector.')[1]: v for k, v in seq_projector_weights.items() if 'mm_seq_projector' in k}\n",
    "            )\n",
    "\n",
    "    def initialize_str_modules(self, model_args, fsdp=None):\n",
    "        self.config.mm_struc_tower = model_args.struc_tower\n",
    "        self.config.mm_str_select_layer = model_args.mm_str_select_layer\n",
    "        self.config.mm_str_select_feature = model_args.mm_str_select_feature\n",
    "        self.config.use_mm_str_proj = True\n",
    "\n",
    "        if self.get_struc_tower() is None:\n",
    "            struc_tower = build_struc_tower(model_args)\n",
    "            self.struc_tower = [struc_tower] if fsdp else struc_tower\n",
    "        else:\n",
    "            struc_tower = self.struc_tower[0] if fsdp else self.struc_tower\n",
    "            struc_tower.load_model()\n",
    "\n",
    "        self.config.mm_str_hidden_size = struc_tower.hidden_size\n",
    "        self.config.mm_struc_projector_type = getattr(model_args, 'mm_struc_projector_type', 'linear')\n",
    "\n",
    "        if getattr(self, 'mm_struc_projector', None) is None:\n",
    "            self.mm_struc_projector = build_struc_projector(self.config)\n",
    "        else:\n",
    "            for p in self.mm_struc_projector.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        if model_args.pretrain_mm_str_mlp_adapter is not None:\n",
    "            struc_projector_weights = torch.load(model_args.pretrain_mm_str_mlp_adapter, map_location='cpu')\n",
    "            self.mm_struc_projector.load_state_dict(\n",
    "                {k.split('mm_struc_projector.')[1]: v for k, v in struc_projector_weights.items() if 'mm_struc_projector' in k}\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693a5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PannotMetaForCausalLM(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_model(self):\n",
    "        pass\n",
    "\n",
    "    def get_seq_tower(self):\n",
    "        return self.get_model().get_seq_tower()\n",
    "\n",
    "    def get_struc_tower(self):\n",
    "        return self.get_model().get_struc_tower()\n",
    "\n",
    "    def encode_seqs(self, seqs):\n",
    "        seq_features = self.get_seq_tower()(seqs)\n",
    "        if seq_features.device != self.device:\n",
    "            seq_features = seq_features.to(self.device)\n",
    "        seq_features = self.get_model().mm_seq_projector(seq_features)\n",
    "        return seq_features\n",
    "\n",
    "    def encode_strs(self, strsp, chain=None):\n",
    "        str_features = self.get_struc_tower()(strsp, chain=chain)\n",
    "        if str_features.device != self.device:\n",
    "            str_features = str_features.to(self.device)\n",
    "        str_features = self.get_model().mm_struc_projector(str_features)\n",
    "        return str_features\n",
    "\n",
    "    def prepare_inputs_labels_for_multimodal(\n",
    "        self, input_ids, position_ids, attention_mask, past_key_values, labels,\n",
    "        seqs=None, strs=None\n",
    "    ):\n",
    "        seq_tower = self.get_seq_tower()\n",
    "        struc_tower = self.get_struc_tower()\n",
    "\n",
    "        _labels = labels\n",
    "        _position_ids = position_ids\n",
    "        _attention_mask = attention_mask\n",
    "\n",
    "\n",
    "        if seq_tower is None and struc_tower is None or input_ids.shape[1] == 1:\n",
    "            return input_ids, position_ids, attention_mask, past_key_values, None, labels\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "        else:\n",
    "            attention_mask = attention_mask.bool()\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(0, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        if labels is None:\n",
    "            labels = torch.full_like(input_ids, IGNORE_INDEX)\n",
    "\n",
    "        # TODO: seq start / end and str start/end is not implemented here to support pretraining.\n",
    "        if getattr(self.config, 'tune_mm_mlp_adapter', False) and getattr(self.config, 'mm_use_seq_start_end', False) and getattr(self.config, 'mm_use_str_start_end', False):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Remove padding\n",
    "        _input_ids = input_ids\n",
    "        input_ids = [cur_input_ids[cur_attention_mask] for cur_input_ids, cur_attention_mask in zip(input_ids, attention_mask)]\n",
    "        labels = [cur_labels[cur_attention_mask] for cur_labels, cur_attention_mask in zip(labels, attention_mask)]\n",
    "\n",
    "        new_input_embeds = []\n",
    "        new_labels = []\n",
    "        cur_seq_idx = 0\n",
    "        cur_str_idx = 0\n",
    "\n",
    "        for batch_idx, cur_input_ids in enumerate(input_ids):\n",
    "            cur_labels = labels[batch_idx]\n",
    "            cur_input_embeds_no_special = []\n",
    "            cur_labels_no_special = []\n",
    "\n",
    "            token_indices = {\n",
    "                SEQ_TOKEN_INDEX: torch.where(cur_input_ids == SEQ_TOKEN_INDEX)[0].tolist(),\n",
    "                STR_TOKEN_INDEX: torch.where(cur_input_ids == STR_TOKEN_INDEX)[0].tolist()\n",
    "            }\n",
    "\n",
    "            # Break input around special tokens\n",
    "            all_specials = sorted([(i, 'seq') for i in token_indices[SEQ_TOKEN_INDEX]] +\n",
    "                                  [(i, 'str') for i in token_indices[STR_TOKEN_INDEX]])\n",
    "            all_specials = [(-1, None)] + all_specials + [(cur_input_ids.shape[0], None)]\n",
    "\n",
    "            cur_embed_segments = []\n",
    "            cur_label_segments = []\n",
    "\n",
    "            for i in range(len(all_specials) - 1):\n",
    "                start = all_specials[i][0] + 1\n",
    "                end = all_specials[i + 1][0]\n",
    "                cur_embed_segments.append(self.get_model().embed_tokens(cur_input_ids[start:end]))\n",
    "                cur_label_segments.append(cur_labels[start:end])\n",
    "\n",
    "                # Add special modality features\n",
    "                if all_specials[i + 1][1] == 'seq':\n",
    "                    seq_feature = self.encode_seqs(seqs[cur_seq_idx]).squeeze(0)\n",
    "                    cur_embed_segments.append(seq_feature)\n",
    "                    cur_label_segments.append(torch.full((seq_feature.shape[0],), IGNORE_INDEX, dtype=cur_labels.dtype, device=cur_labels.device))\n",
    "                    \n",
    "                    #     # Embed the start and end tokens (1, D)\n",
    "                    # start_embed = self.get_model().embed_tokens(\n",
    "                    #     torch.tensor([SEQ_START_TOKEN_ID], device=seq_feature.device)\n",
    "                    # )\n",
    "                    # end_embed = self.get_model().embed_tokens(\n",
    "                    #     torch.tensor([SEQ_END_TOKEN_ID], device=seq_feature.device)\n",
    "                    # )\n",
    "\n",
    "                    # # Concatenate to make (L + 2, D)\n",
    "                    # seq_feature = torch.cat([start_embed, seq_feature, end_embed], dim=0)\n",
    "\n",
    "                    # # Create dummy labels for these tokens (set to IGNORE_INDEX)\n",
    "                    # seq_labels = torch.full((seq_feature.shape[0],), IGNORE_INDEX, dtype=cur_labels.dtype, device=cur_labels.device)\n",
    "\n",
    "                    # cur_embed_segments.append(seq_feature)\n",
    "                    # cur_label_segments.append(seq_labels)\n",
    "                                    \n",
    "                    cur_seq_idx += 1\n",
    "                elif all_specials[i + 1][1] == 'str':\n",
    "                    str_feature = self.encode_strs(strs[cur_str_idx]).squeeze(0)\n",
    "                    cur_embed_segments.append(str_feature)\n",
    "                    cur_label_segments.append(torch.full((str_feature.shape[0],), IGNORE_INDEX, dtype=cur_labels.dtype, device=cur_labels.device))\n",
    "                    cur_str_idx += 1\n",
    "\n",
    "            final_embed = torch.cat(cur_embed_segments, dim=0).to(self.device)\n",
    "            final_labels = torch.cat(cur_label_segments, dim=0).to(self.device)\n",
    "\n",
    "            new_input_embeds.append(final_embed)\n",
    "            new_labels.append(final_labels)\n",
    "\n",
    "        # Truncate and pad\n",
    "        tokenizer_model_max_length = getattr(self.config, 'tokenizer_model_max_length', None)\n",
    "        if tokenizer_model_max_length is not None:\n",
    "            new_input_embeds = [x[:tokenizer_model_max_length] for x in new_input_embeds]\n",
    "            new_labels = [x[:tokenizer_model_max_length] for x in new_labels]\n",
    "\n",
    "        max_len = max(x.shape[0] for x in new_input_embeds)\n",
    "        batch_size = len(new_input_embeds)\n",
    "\n",
    "        new_input_embeds_padded = []\n",
    "        new_labels_padded = torch.full((batch_size, max_len), IGNORE_INDEX, dtype=new_labels[0].dtype, device=new_labels[0].device)\n",
    "        attention_mask = torch.zeros((batch_size, max_len), dtype=torch.bool, device=new_labels[0].device)\n",
    "        position_ids = torch.zeros((batch_size, max_len), dtype=torch.long, device=new_labels[0].device)\n",
    "\n",
    "        for i, (emb, lab) in enumerate(zip(new_input_embeds, new_labels)):\n",
    "            cur_len = emb.shape[0]\n",
    "            padding_side = getattr(self.config, 'tokenizer_padding_side', 'right')\n",
    "\n",
    "            if padding_side == \"left\":\n",
    "                padded = torch.cat([torch.zeros((max_len - cur_len, emb.shape[1]), device=emb.device), emb], dim=0)\n",
    "                new_input_embeds_padded.append(padded)\n",
    "                new_labels_padded[i, -cur_len:] = lab\n",
    "                attention_mask[i, -cur_len:] = True\n",
    "                position_ids[i, -cur_len:] = torch.arange(cur_len, device=emb.device)\n",
    "            else:\n",
    "                padded = torch.cat([emb, torch.zeros((max_len - cur_len, emb.shape[1]), device=emb.device)], dim=0)\n",
    "                new_input_embeds_padded.append(padded)\n",
    "                new_labels_padded[i, :cur_len] = lab\n",
    "                attention_mask[i, :cur_len] = True\n",
    "                position_ids[i, :cur_len] = torch.arange(cur_len, device=emb.device)\n",
    "\n",
    "        new_input_embeds = torch.stack(new_input_embeds_padded, dim=0)\n",
    "\n",
    "        # 如果原始标签为空，则将新的标签设置为空\n",
    "        if _labels is None:\n",
    "            new_labels = None\n",
    "        else:\n",
    "            new_labels = new_labels_padded\n",
    "\n",
    "        # 如果原始注意力掩码为空，则将新的注意力掩码设置为空\n",
    "        if _attention_mask is None:\n",
    "            attention_mask = None\n",
    "        else:\n",
    "            attention_mask = attention_mask.to(dtype=_attention_mask.dtype)\n",
    "\n",
    "        # 如果原始位置id为空，则将新的位置id设置为空\n",
    "        if _position_ids is None:\n",
    "            position_ids = None\n",
    "\n",
    "        return None, position_ids, attention_mask, past_key_values, new_input_embeds, new_labels_padded\n",
    "    \n",
    "    def initialize_seq_tokenizer(self, model_args, tokenizer):\n",
    "        if model_args.mm_use_seq_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_SEQ_PATCH_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if model_args.mm_use_seq_start_end:\n",
    "            num_new_tokens = tokenizer.add_tokens(\n",
    "                [DEFAULT_SEQ_START_TOKEN, DEFAULT_SEQ_END_TOKEN], special_tokens=True\n",
    "            )\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            if num_new_tokens > 0:\n",
    "                input_embeddings = self.get_input_embeddings().weight.data\n",
    "                output_embeddings = self.get_output_embeddings().weight.data\n",
    "\n",
    "                input_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "                output_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "                input_embeddings[-num_new_tokens:] = input_avg\n",
    "                output_embeddings[-num_new_tokens:] = output_avg\n",
    "\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "            if model_args.pretrain_mm_mlp_adapter:\n",
    "                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n",
    "                assert num_new_tokens == 2\n",
    "                if input_embeddings.shape == embed_tokens_weight.shape:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n",
    "                elif embed_tokens_weight.shape[0] == num_new_tokens:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unexpected embed_tokens_weight shape. \"\n",
    "                        f\"Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. \"\n",
    "                        f\"Number of new tokens: {num_new_tokens}.\"\n",
    "                    )\n",
    "        elif model_args.mm_use_seq_patch_token:\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "    def initialize_str_tokenizer(self, model_args, tokenizer):\n",
    "        if model_args.mm_use_str_patch_token:\n",
    "            tokenizer.add_tokens([DEFAULT_STR_PATCH_TOKEN], special_tokens=True)\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        if model_args.mm_use_str_start_end:\n",
    "            num_new_tokens = tokenizer.add_tokens(\n",
    "                [DEFAULT_STR_START_TOKEN, DEFAULT_STR_END_TOKEN], special_tokens=True\n",
    "            )\n",
    "            self.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "            if num_new_tokens > 0:\n",
    "                input_embeddings = self.get_input_embeddings().weight.data\n",
    "                output_embeddings = self.get_output_embeddings().weight.data\n",
    "\n",
    "                input_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "                output_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "                input_embeddings[-num_new_tokens:] = input_avg\n",
    "                output_embeddings[-num_new_tokens:] = output_avg\n",
    "\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = True\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n",
    "            if model_args.pretrain_mm_mlp_adapter:\n",
    "                mm_projector_weights = torch.load(model_args.pretrain_mm_mlp_adapter, map_location='cpu')\n",
    "                embed_tokens_weight = mm_projector_weights['model.embed_tokens.weight']\n",
    "                assert num_new_tokens == 2\n",
    "                if input_embeddings.shape == embed_tokens_weight.shape:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight[-num_new_tokens:]\n",
    "                elif embed_tokens_weight.shape[0] == num_new_tokens:\n",
    "                    input_embeddings[-num_new_tokens:] = embed_tokens_weight\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unexpected embed_tokens_weight shape. \"\n",
    "                        f\"Pretrained: {embed_tokens_weight.shape}. Current: {input_embeddings.shape}. \"\n",
    "                        f\"Number of new tokens: {num_new_tokens}.\"\n",
    "                    )\n",
    "        elif model_args.mm_use_str_patch_token:\n",
    "            if model_args.tune_mm_mlp_adapter:\n",
    "                for p in self.get_input_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "                for p in self.get_output_embeddings().parameters():\n",
    "                    p.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b9c5c",
   "metadata": {},
   "source": [
    "## Pannot_llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04b95020",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, \\\n",
    "                         LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers.generation.utils import GenerateOutput\n",
    "\n",
    "# from ..pannot_arch import PannotMetaModel, PannotMetaForCausalLM\n",
    "\n",
    "\n",
    "# from ..pannot_arch import PannotMetaModel, PannotMetaForCausalLM\n",
    "\n",
    "class PannotConfig(LlamaConfig):\n",
    "    model_type = \"pannot_llama\"\n",
    "\n",
    "\n",
    "class PannotLlamaModel(PannotMetaModel, LlamaModel):\n",
    "    config_class = PannotConfig\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "class PannotLlamaForCausalLM(LlamaForCausalLM, PannotMetaForCausalLM):\n",
    "    config_class = PannotConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(LlamaForCausalLM, self).__init__(config)\n",
    "        self.model = PannotLlamaModel(config)\n",
    "        self.pretraining_tp = config.pretraining_tp\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        seqs: Optional[torch.FloatTensor] = None,  # (B, L_seq, D)\n",
    "        strs: Optional[torch.FloatTensor] = None,  # (B, L_str, D)\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "        \n",
    "        kwargs.pop('cache_position', None)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                inputs_embeds,\n",
    "                labels\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                past_key_values,\n",
    "                labels,\n",
    "                seqs,\n",
    "                strs\n",
    "            )\n",
    "\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        seqs: Optional[torch.FloatTensor] = None,\n",
    "        strs: Optional[torch.FloatTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
    "        kwargs.pop('cache_position', None)\n",
    "\n",
    "        if seqs is not None or strs is not None:\n",
    "            (\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                _,\n",
    "                inputs_embeds,\n",
    "                _\n",
    "            ) = self.prepare_inputs_labels_for_multimodal(\n",
    "                input_ids,\n",
    "                position_ids,\n",
    "                attention_mask,\n",
    "                None,\n",
    "                None,\n",
    "                seqs,\n",
    "                strs\n",
    "            )\n",
    "        else:\n",
    "            inputs_embeds = self.get_model().embed_tokens(input_ids)\n",
    "\n",
    "        return super().generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        seqs = kwargs.pop(\"seqs\", None)\n",
    "        strs = kwargs.pop(\"strs\", None)\n",
    "        inputs = super().prepare_inputs_for_generation(\n",
    "            input_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            **kwargs\n",
    "        )\n",
    "        if seqs is not None:\n",
    "            inputs[\"seqs\"] = seqs\n",
    "        if strs is not None:\n",
    "            inputs[\"strs\"] = strs\n",
    "        return inputs\n",
    "\n",
    "\n",
    "AutoConfig.register(\"pannot_llama\", PannotConfig)\n",
    "AutoModelForCausalLM.register(PannotConfig, PannotLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1cb947b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type pannot_llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# Send the embeddings to the PannotLlamaForCausalLM\n",
    "# Load TinyLlama config and tokenizer\n",
    "pretrained_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "config = PannotConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799275b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PannotConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"pannot_llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7da9c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing multimodal fields manually\n",
    "config.mm_seq_tower = \"ESM\"\n",
    "config.mm_struc_tower = \"ESMIF\"\n",
    "config.mm_seq_hidden_size = 1280\n",
    "\n",
    "config.mm_struc_hidden_size = 512\n",
    "config.mm_seq_projector_type = \"linear\"\n",
    "config.mm_struc_projector_type = \"linear\"\n",
    "config.mm_seq_select_layer = -1\n",
    "config.mm_seq_select_feature = \"cls\"\n",
    "config.mm_str_select_layer = -1\n",
    "config.mm_str_select_feature = \"mean\"\n",
    "config.use_mm_seq_proj = True\n",
    "config.use_mm_str_proj = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1c640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b82e2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PannotConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 5632,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"mlp_bias\": false,\n",
       "  \"mm_seq_hidden_size\": 1280,\n",
       "  \"mm_seq_projector_type\": \"linear\",\n",
       "  \"mm_seq_select_feature\": \"cls\",\n",
       "  \"mm_seq_select_layer\": -1,\n",
       "  \"mm_seq_tower\": \"ESM\",\n",
       "  \"mm_str_select_feature\": \"mean\",\n",
       "  \"mm_str_select_layer\": -1,\n",
       "  \"mm_struc_hidden_size\": 512,\n",
       "  \"mm_struc_projector_type\": \"linear\",\n",
       "  \"mm_struc_tower\": \"ESMIF\",\n",
       "  \"model_type\": \"pannot_llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 22,\n",
       "  \"num_key_value_heads\": 4,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.48.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"use_mm_seq_proj\": true,\n",
       "  \"use_mm_str_proj\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce207df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = PannotLlamaForCausalLM(config)\n",
    "# model.load_state_dict(torch.load(\"tinyllama_pannot_weights.pt\", map_location=\"cpu\"))  # Optional, if you have weights\n",
    "\n",
    "# Move model and data to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361e633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.mm_seq_hidden_size = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc613fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 78, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Load test sequence\n",
    "sequence = model.get_seq_tower().load_fasta_sequence(\"../asset/demo_seq_str/pdb_1ubq/rcsb_pdb_1UBQ.fasta\")\n",
    "# sequence = sequence.to(device)\n",
    "# Run through encoder\n",
    "with torch.no_grad():\n",
    "    output = model.encode_seqs(sequence)  # Note: input is a list of sequences\n",
    "    # seq_features = model.get_seq_tower()([sequence])\n",
    "    # print(seq_features.device)\n",
    "    # print(model.get_seq_tower().device)\n",
    "    # # seq_features = model.get_model().mm_seq_projector(seq_features)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # (1, hidden_size) or (1, seq_len, hidden_size) if no_pooling=True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "566926e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1026, -0.1568,  0.0708,  ...,  0.0030, -0.1666,  0.2931],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7055bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure emb shape: torch.Size([1, 76, 2048])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify the path to your PDB or mmCIF file\n",
    "structure_path = \"../asset/demo_seq_str/pdb_1ubq/1ubq.cif\"\n",
    "\n",
    "# Optionally, specify the chain of interest\n",
    "chain = \"A\"\n",
    "\n",
    "# structure = model.get_struc_tower().load_structure(structure_path, chain)\n",
    "# Get the protein embeddings\n",
    "\n",
    "with torch.no_grad():    \n",
    "    str_embeddings = model.encode_strs(structure_path, chain = chain)\n",
    "\n",
    "print(\"Structure emb shape:\", str_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac0b1a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MQIFVKTLTGKTITLEVEPSDTIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9400585",
   "metadata": {},
   "source": [
    "## Modify the tokenizer to include custom tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84ab4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s></s> This I provided:seq> <'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor([1,2,910, 306,  4944, 29901,11762, 29958,   529,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1d6c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenizer_protein_token(prompt, tokenizer, seq_token_index=SEQ_TOKEN_INDEX, str_token_index=STR_TOKEN_INDEX, return_tensors=None):\n",
    "    # Split the prompt on both <seq> and <str> while preserving the split tokens\n",
    "    prompt_chunks = re.split(r'(<seq>|<str>)', prompt)\n",
    "\n",
    "    # Tokenize the chunks and replace <seq> and <str> with their respective token indices\n",
    "    tokenized_input = []\n",
    "    for chunk in prompt_chunks:\n",
    "        if chunk == '<seq>':\n",
    "            tokenized_input.append(seq_token_index)\n",
    "        elif chunk == '<str>':\n",
    "            tokenized_input.append(str_token_index)\n",
    "        else:\n",
    "            # Tokenize the chunk normally\n",
    "            tokenized_input.extend(tokenizer.encode(chunk, add_special_tokens=False))\n",
    "\n",
    "    # If return_tensors is specified, return the result as a PyTorch tensor\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(tokenized_input, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0e8a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict,\n",
    "    tokenizer,\n",
    "    model,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc38c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the special tokens you want to add\n",
    "# special_tokens_dict = {\n",
    "#     'additional_special_tokens': [\n",
    "#         \"<seq>\", \"<seq_patch>\", \"<seq_start>\", \"<seq_end>\",\n",
    "#         \"<str>\", \"<str_patch>\", \"<str_start>\", \"<str_end>\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Assuming you already have the tokenizer and model objects defined\n",
    "# smart_tokenizer_and_embedding_resize(special_tokens_dict, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3808e1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[910,\n",
       " 338,\n",
       " 263,\n",
       " 2107,\n",
       " 26823,\n",
       " 306,\n",
       " 864,\n",
       " 304,\n",
       " 6559,\n",
       " 29901,\n",
       " 29871,\n",
       " -330,\n",
       " 259,\n",
       " -360]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_protein_token(\"This is a great protein I want to study: <seq> <str>\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5bd1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'mm_use_seq_patch_token': False,\n",
    "    'mm_use_seq_start_end': False,\n",
    "    'mm_use_str_patch_token': False,\n",
    "    'mm_use_str_start_end': False,\n",
    "    'tune_mm_mlp_adapter': True,\n",
    "    'pretrain_mm_mlp_adapter': 'path_to_pretrained_mlp_adapter.pth'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "957dd6b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'mm_use_seq_patch_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39minitialize_seq_tokenizer(model_args, tokenizer)\n",
      "Cell \u001b[0;32mIn[13], line 179\u001b[0m, in \u001b[0;36mPannotMetaForCausalLM.initialize_seq_tokenizer\u001b[0;34m(self, model_args, tokenizer)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_seq_tokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_args, tokenizer):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_args\u001b[38;5;241m.\u001b[39mmm_use_seq_patch_token:\n\u001b[1;32m    180\u001b[0m         tokenizer\u001b[38;5;241m.\u001b[39madd_tokens([DEFAULT_SEQ_PATCH_TOKEN], special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'mm_use_seq_patch_token'"
     ]
    }
   ],
   "source": [
    "model.initialize_seq_tokenizer(model_args, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f2a5ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "out of range integral type conversion attempted",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdecode(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:3851\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3849\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3852\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3853\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3854\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3855\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3856\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch_env/lib/python3.12/site-packages/transformers/tokenization_utils_fast.py:668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    667\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 668\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[1;32m    670\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    671\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(torch.tensor([-1, -2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7331e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invånaregetElementById invånareѝ invånareѝ/~ Theoryு)\" ufficiale participe ufficiale participe)\" ufficiale participe)\" Bagுுுுுுுae ufficiale présent ufficialegetElementByIdѝ rid participe ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale participeionales coalptrтельства xmlns més ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale载⠀dependenciesakedAfterAfterAfterAfterAfterAfterAfterAfterAfterAfterAfterAfter ufficiale intersect senior ufficiale intersect senior ufficialeÞ ufficialeÞ musesubscribeAfter ufficiale ufficiale ufficiale ufficiale ufficiale ufficiale ufficialeAfterční\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare input IDs\n",
    "input_ids = tokenizer_protein_token(\"<s> This is the protein I provided: <seq> <str> Could you introduce its information?\", tokenizer,return_tensors='pt').unsqueeze(0).cuda()\n",
    "\n",
    "print(input_ids.shape)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        seqs=[sequence],\n",
    "        strs=[structure_path],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee3fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adc3c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
