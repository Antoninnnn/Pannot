{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "from torch.utils.data import Dataset\n",
    "from enum import auto, Enum\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "from typing import List, Union,Tuple, Optional,Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "IS_TOKENIZER_GREATER_THAN_0_14 = version.parse(tokenizers.__version__) >= version.parse('0.14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IS_TOKENIZER_GREATER_THAN_0_14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    # Core model\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "\n",
    "    # Global control\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "\n",
    "    # ===== SEQUENCE TOWER =====\n",
    "    use_seq_tower: bool = field(default=True)\n",
    "    mm_seq_tower: Optional[str] = field(default=\"ESM\")  # One of: \"ProtST\", \"ESM\"\n",
    "    mm_seq_select_layer: Optional[int] = field(default=-1)\n",
    "    mm_seq_projector_type: Optional[str] = field(default=\"linear\")\n",
    "    mm_use_seq_start_end: bool = field(default=False)\n",
    "    mm_use_seq_patch_token: bool = field(default=False)\n",
    "\n",
    "    # ===== STRUCTURE TOWER =====\n",
    "    use_str_tower: bool = field(default=True)\n",
    "    mm_struc_tower: Optional[str] = field(default=\"ESM3\")  # One of: \"ESMIF\", \"ESM3\"\n",
    "    mm_str_projector_type: Optional[str] = field(default=\"linear\")\n",
    "    mm_use_str_start_end: bool = field(default=False)\n",
    "    mm_use_str_patch_token: bool = field(default=False)\n",
    "\n",
    "    # ===== Fusion control (optional) =====\n",
    "    mm_fusion_type: Optional[str] = field(default=\"concat\")  # e.g., \"concat\", \"sum\", \"crossattn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SeparatorStyle(Enum):\n",
    "    \"\"\"Different separator style.\"\"\"\n",
    "    SINGLE = auto()  \n",
    "    TWO = auto()  \n",
    "    MPT = auto()  \n",
    "    PLAIN = auto()  \n",
    "    LLAMA_2 = auto()  \n",
    "\n",
    "@dataclass\n",
    "class ProteinInput:\n",
    "    \"\"\"Class to represent a protein input with sequence, structure, and annotations.\"\"\"\n",
    "    sequence: str  # Amino acid sequence\n",
    "    structure: Optional[str] = None  # e.g., path to PDB file or structural data\n",
    "    annotations: Optional[str] = None  # Functional annotations or descriptions\n",
    "\n",
    "@dataclass\n",
    "class Conversation:\n",
    "    system: str\n",
    "    roles: List[str]\n",
    "    messages: List[List[Union[str, ProteinInput]]]\n",
    "    offset: int = 0\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.SINGLE\n",
    "    sep: str = \"###\"\n",
    "    sep2: Optional[str] = None\n",
    "    version: str = \"Pannot\"\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        messages = self.messages\n",
    "        if self.sep_style == SeparatorStyle.SINGLE:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                ret += role + \": \" + self._format_message(message) + self.sep\n",
    "        elif self.sep_style == SeparatorStyle.TWO:\n",
    "            seps = [self.sep, self.sep2 or self.sep]\n",
    "            ret = self.system + seps[0]\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                ret += role + \": \" + self._format_message(message) + seps[i % 2]\n",
    "        elif self.sep_style == SeparatorStyle.PLAIN:\n",
    "            ret = self.system + \"\\n\"\n",
    "            for role, message in messages:\n",
    "                ret += role + \": \" + self._format_message(message) + \"\\n\"\n",
    "        elif self.sep_style == SeparatorStyle.LLAMA_2:\n",
    "            wrap_sys = lambda msg: f\"<<SYS>>\\n{msg}\\n<</SYS>>\\n\\n\" if msg else \"\"\n",
    "            wrap_inst = lambda msg: f\"[INST] {msg} [/INST]\"\n",
    "            ret = \"\"\n",
    "            for i, (role, message) in enumerate(messages):\n",
    "                formatted = self._format_message(message)\n",
    "                if i == 0:\n",
    "                    assert role == self.roles[0], \"First message must be from user\"\n",
    "                    formatted = wrap_sys(self.system) + formatted\n",
    "                if i % 2 == 0:\n",
    "                    ret += self.sep + wrap_inst(formatted)\n",
    "                else:\n",
    "                    ret += \" \" + formatted + \" \" + (self.sep2 or self.sep)\n",
    "            ret = ret.lstrip(self.sep)\n",
    "        elif self.sep_style == SeparatorStyle.MPT:\n",
    "            ret = self.system + self.sep\n",
    "            for role, message in messages:\n",
    "                ret += role + self._format_message(message) + self.sep\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid separator style: {self.sep_style}\")\n",
    "        return ret\n",
    "\n",
    "    def _format_message(self, message: Union[str, ProteinInput]) -> str:\n",
    "        if isinstance(message, ProteinInput):\n",
    "            lines = [f\"<seq> {message.sequence} </seq>\"]\n",
    "            if message.structure:\n",
    "                lines.append(f\"<str> {message.structure} </str>\")\n",
    "            if message.annotations:\n",
    "                lines.append(f\"<anno> {message.annotations} </anno>\")\n",
    "            return \"\\n\".join(lines)\n",
    "        return message\n",
    "\n",
    "    def append_message(self, role: str, message: Union[str, ProteinInput]):\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def copy(self) -> 'Conversation':\n",
    "        return Conversation(\n",
    "            system=self.system,\n",
    "            roles=self.roles,\n",
    "            messages=[[r, m] for r, m in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            version=self.version,\n",
    "        )\n",
    "\n",
    "default_conversation = Conversation(\n",
    "    system=\"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "    roles=[\"USER\", \"ASSISTANT\"],\n",
    "    version=\"v1\",\n",
    "    messages=(),\n",
    "    offset=0,\n",
    "    sep_style=SeparatorStyle.TWO,\n",
    "    sep=\" \",\n",
    "    sep2=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \""
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_conversation.get_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "\n",
    "PROT_TOKEN_INDEX = -300\n",
    "DEFAULT_PROT_TOKEN = \"<prot>\"\n",
    "DEFAULT_PROT_PATCH_TOKEN = \"<prot_patch>\"\n",
    "DEFAULT_PROT_START_TOKEN = \"<prot_start>\"\n",
    "DEFAULT_PROT_END_TOKEN = \"<prot_end>\"\n",
    "PROT_PLACEHOLDER = \"<prot-placeholder>\"\n",
    "\n",
    "SEQ_TOKEN_INDEX = -330\n",
    "DEFAULT_SEQ_TOKEN = \"<seq>\"\n",
    "DEFAULT_SEQ_PATCH_TOKEN = \"<seq_patch>\"\n",
    "DEFAULT_SEQ_START_TOKEN = \"<seq_start>\"\n",
    "DEFAULT_SEQ_END_TOKEN = \"<seq_end>\"\n",
    "\n",
    "STR_TOKEN_INDEX = -360\n",
    "DEFAULT_STR_TOKEN = \"<str>\"\n",
    "DEFAULT_STR_PATCH_TOKEN = \"<str_patch>\"\n",
    "DEFAULT_STR_START_TOKEN = \"<str_start>\"\n",
    "DEFAULT_STR_END_TOKEN = \"<str_end>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    seq_folder: Optional[str] = field(default=None)\n",
    "    struc_folder: Optional[str] = field(default=None)\n",
    "    # image_aspect_ratio: str = 'square'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_multimodal(\n",
    "    sources: Sequence[str],\n",
    "    data_args: DataArguments\n",
    ") -> Dict:\n",
    "    if not data_args.is_multimodal:\n",
    "        return sources\n",
    "\n",
    "    use_seq_start_end = getattr(data_args, \"use_seq_start_end\", False)\n",
    "    use_str_start_end = getattr(data_args, \"use_str_start_end\", False)\n",
    "\n",
    "    for source in sources:\n",
    "        for sentence in source:\n",
    "            value = sentence['value']\n",
    "\n",
    "            # Handle <seq> tokens\n",
    "            if DEFAULT_SEQ_TOKEN in value:\n",
    "                value = value.replace(DEFAULT_SEQ_TOKEN, '').strip()\n",
    "                value = DEFAULT_SEQ_TOKEN + '\\n' + value\n",
    "                if use_seq_start_end:\n",
    "                    value = value.replace(\n",
    "                        DEFAULT_SEQ_TOKEN,\n",
    "                        DEFAULT_SEQ_START_TOKEN + DEFAULT_SEQ_TOKEN + DEFAULT_SEQ_END_TOKEN\n",
    "                    )\n",
    "\n",
    "            # Handle <str> tokens\n",
    "            if DEFAULT_STR_TOKEN in value:\n",
    "                value = value.replace(DEFAULT_STR_TOKEN, '').strip()\n",
    "                value = DEFAULT_STR_TOKEN + '\\n' + value\n",
    "                if use_str_start_end:\n",
    "                    value = value.replace(\n",
    "                        DEFAULT_STR_TOKEN,\n",
    "                        DEFAULT_STR_START_TOKEN + DEFAULT_STR_TOKEN + DEFAULT_STR_END_TOKEN\n",
    "                    )\n",
    "\n",
    "            sentence['value'] = value.strip()\n",
    "\n",
    "    return sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenizer_protein_token(prompt, tokenizer, seq_token_index=SEQ_TOKEN_INDEX, str_token_index=STR_TOKEN_INDEX, return_tensors=None):\n",
    "    # Split the prompt on both <seq> and <str> while preserving the split tokens\n",
    "    prompt_chunks = re.split(r'(<seq>|<str>)', prompt)\n",
    "\n",
    "    # Tokenize the chunks and replace <seq> and <str> with their respective token indices\n",
    "    tokenized_input = []\n",
    "    for chunk in prompt_chunks:\n",
    "        if chunk == '<seq>':\n",
    "            tokenized_input.append(seq_token_index)\n",
    "        elif chunk == '<str>':\n",
    "            tokenized_input.append(str_token_index)\n",
    "        else:\n",
    "            # Tokenize the chunk normally\n",
    "            tokenized_input.extend(tokenizer.encode(chunk, add_special_tokens=False))\n",
    "\n",
    "    # If return_tensors is specified, return the result as a PyTorch tensor\n",
    "    if return_tensors is not None:\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(tokenized_input, dtype=torch.long)\n",
    "        raise ValueError(f'Unsupported tensor type: {return_tensors}')\n",
    "\n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def preprocess_llama_2(\n",
    "#     sources,\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "#     has_image: bool = False\n",
    "# ) -> Dict:\n",
    "#     conv = default_conversation.copy()\n",
    "#     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "#     # Apply prompt templates\n",
    "#     conversations = []\n",
    "#     for i, source in enumerate(sources):\n",
    "#         if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "#             # Skip the first one if it is not from human\n",
    "#             source = source[1:]\n",
    "\n",
    "#         conv.messages = []\n",
    "#         for j, sentence in enumerate(source):\n",
    "#             role = roles[sentence[\"from\"]]\n",
    "#             assert role == conv.roles[j % 2], f\"{i}\"\n",
    "#             conv.append_message(role, sentence[\"value\"])\n",
    "#         conversations.append(conv.get_prompt())\n",
    "\n",
    "#     # Tokenize conversations\n",
    "\n",
    "#     if has_image:\n",
    "#         input_ids = torch.stack([tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "#     else:\n",
    "#         input_ids = tokenizer(\n",
    "#             conversations,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=\"longest\",\n",
    "#             max_length=tokenizer.model_max_length,\n",
    "#             truncation=True,\n",
    "#         ).input_ids\n",
    "\n",
    "#     targets = input_ids.clone()\n",
    "\n",
    "#     assert conv.sep_style == SeparatorStyle.LLAMA_2\n",
    "\n",
    "#     # Mask targets\n",
    "#     sep = \"[/INST] \"\n",
    "#     for conversation, target in zip(conversations, targets):\n",
    "#         total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "#         rounds = conversation.split(conv.sep2)\n",
    "#         cur_len = 1\n",
    "#         target[:cur_len] = IGNORE_INDEX\n",
    "#         for i, rou in enumerate(rounds):\n",
    "#             if rou == \"\":\n",
    "#                 break\n",
    "\n",
    "#             parts = rou.split(sep)\n",
    "#             if len(parts) != 2:\n",
    "#                 break\n",
    "#             parts[0] += sep\n",
    "\n",
    "#             if has_image:\n",
    "#                 round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "#                 instruction_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 2\n",
    "#             else:\n",
    "#                 round_len = len(tokenizer(rou).input_ids)\n",
    "#                 instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "#             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "#             cur_len += round_len\n",
    "#         target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "#         if cur_len < tokenizer.model_max_length:\n",
    "#             if cur_len != total_len:\n",
    "#                 target[:] = IGNORE_INDEX\n",
    "#                 print(\n",
    "#                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "#                     f\" (ignored)\"\n",
    "#                 )\n",
    "\n",
    "#     return dict(\n",
    "#         input_ids=input_ids,\n",
    "#         labels=targets,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def preprocess_v1(\n",
    "#     sources,\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "#     has_image: bool = False\n",
    "# ) -> Dict:\n",
    "#     conv = default_conversation.copy()\n",
    "#     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "#     # Apply prompt templates\n",
    "#     conversations = []\n",
    "#     for i, source in enumerate(sources):\n",
    "#         if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "#             # Skip the first one if it is not from human\n",
    "#             source = source[1:]\n",
    "\n",
    "#         conv.messages = []\n",
    "#         for j, sentence in enumerate(source):\n",
    "#             role = roles[sentence[\"from\"]]\n",
    "#             assert role == conv.roles[j % 2], f\"{i}\"\n",
    "#             conv.append_message(role, sentence[\"value\"])\n",
    "#         conversations.append(conv.get_prompt())\n",
    "\n",
    "#     # Tokenize conversations\n",
    "\n",
    "#     if has_image:\n",
    "#         input_ids = torch.stack([tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "#     else:\n",
    "#         input_ids = tokenizer(\n",
    "#             conversations,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=\"longest\",\n",
    "#             max_length=tokenizer.model_max_length,\n",
    "#             truncation=True,\n",
    "#         ).input_ids\n",
    "\n",
    "#     targets = input_ids.clone()\n",
    "\n",
    "#     assert conv.sep_style == SeparatorStyle.TWO\n",
    "\n",
    "#     # Mask targets\n",
    "#     sep = conv.sep + conv.roles[1] + \": \"\n",
    "#     for conversation, target in zip(conversations, targets):\n",
    "#         total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "#         rounds = conversation.split(conv.sep2)\n",
    "#         cur_len = 1\n",
    "#         target[:cur_len] = IGNORE_INDEX\n",
    "#         for i, rou in enumerate(rounds):\n",
    "#             if rou == \"\":\n",
    "#                 break\n",
    "\n",
    "#             parts = rou.split(sep)\n",
    "#             if len(parts) != 2:\n",
    "#                 break\n",
    "#             parts[0] += sep\n",
    "\n",
    "#             if has_image:\n",
    "#                 round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "#                 instruction_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 2\n",
    "#             else:\n",
    "#                 round_len = len(tokenizer(rou).input_ids)\n",
    "#                 instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "#             if i != 0 and not tokenizer.legacy and IS_TOKENIZER_GREATER_THAN_0_14:\n",
    "#                 round_len -= 1\n",
    "#                 instruction_len -= 1\n",
    "\n",
    "#             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "#             cur_len += round_len\n",
    "#         target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "#         if cur_len < tokenizer.model_max_length:\n",
    "#             if cur_len != total_len:\n",
    "#                 target[:] = IGNORE_INDEX\n",
    "#                 print(\n",
    "#                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "#                     f\" (ignored)\"\n",
    "#                 )\n",
    "\n",
    "#     return dict(\n",
    "#         input_ids=input_ids,\n",
    "#         labels=targets,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def preprocess_mpt(\n",
    "#     sources,\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "#     has_image: bool = False\n",
    "# ) -> Dict:\n",
    "#     conv = default_conversation.copy()\n",
    "#     roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "#     # Apply prompt templates\n",
    "#     conversations = []\n",
    "#     for i, source in enumerate(sources):\n",
    "#         if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "#             # Skip the first one if it is not from human\n",
    "#             source = source[1:]\n",
    "\n",
    "#         conv.messages = []\n",
    "#         for j, sentence in enumerate(source):\n",
    "#             role = roles[sentence[\"from\"]]\n",
    "#             assert role == conv.roles[j % 2], f\"{i}\"\n",
    "#             conv.append_message(role, sentence[\"value\"])\n",
    "#         conversations.append(conv.get_prompt())\n",
    "\n",
    "#     # Tokenize conversations\n",
    "\n",
    "#     if has_image:\n",
    "#         input_ids = torch.stack([tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0)\n",
    "#     else:\n",
    "#         input_ids = tokenizer(\n",
    "#             conversations,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=\"longest\",\n",
    "#             max_length=tokenizer.model_max_length,\n",
    "#             truncation=True,\n",
    "#         ).input_ids\n",
    "\n",
    "#     targets = input_ids.clone()\n",
    "#     assert conv.sep_style == SeparatorStyle.MPT\n",
    "\n",
    "#     # Mask targets\n",
    "#     sep = conv.sep + conv.roles[1]\n",
    "#     for conversation, target in zip(conversations, targets):\n",
    "#         total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "#         rounds = conversation.split(conv.sep)\n",
    "#         re_rounds = [conv.sep.join(rounds[:3])] # system + user + gpt\n",
    "#         for conv_idx in range(3, len(rounds), 2):\n",
    "#             re_rounds.append(conv.sep.join(rounds[conv_idx:conv_idx+2]))    # user + gpt\n",
    "#         cur_len = 0\n",
    "#         target[:cur_len] = IGNORE_INDEX\n",
    "#         for i, rou in enumerate(re_rounds):\n",
    "#             if rou == \"\":\n",
    "#                 break\n",
    "\n",
    "#             parts = rou.split(sep)\n",
    "#             if len(parts) != 2:\n",
    "#                 break\n",
    "#             parts[0] += sep\n",
    "\n",
    "#             if has_image:\n",
    "#                 round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "#                 instruction_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 1\n",
    "#             else:\n",
    "#                 round_len = len(tokenizer(rou).input_ids)\n",
    "#                 instruction_len = len(tokenizer(parts[0]).input_ids) - 1\n",
    "\n",
    "#             if i != 0 and getattr(tokenizer, 'legacy', False) and IS_TOKENIZER_GREATER_THAN_0_14:\n",
    "#                 round_len += 1\n",
    "#                 instruction_len += 1\n",
    "\n",
    "#             target[cur_len : cur_len + instruction_len] = IGNORE_INDEX\n",
    "\n",
    "#             cur_len += round_len\n",
    "#         target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "#         if cur_len < tokenizer.model_max_length:\n",
    "#             if cur_len != total_len:\n",
    "#                 target[:] = IGNORE_INDEX\n",
    "#                 print(\n",
    "#                     f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "#                     f\" (ignored)\"\n",
    "#                 )\n",
    "\n",
    "#     return dict(\n",
    "#         input_ids=input_ids,\n",
    "#         labels=targets,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def preprocess_plain(\n",
    "#     sources: Sequence[str],\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "# ) -> Dict:\n",
    "#     # add end signal and concatenate together\n",
    "#     conversations = []\n",
    "#     for source in sources:\n",
    "#         assert len(source) == 2\n",
    "#         assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "#         source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "#         conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "#         conversations.append(conversation)\n",
    "#     # tokenize conversations\n",
    "#     input_ids = [tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "#     targets = copy.deepcopy(input_ids)\n",
    "#     for target, source in zip(targets, sources):\n",
    "#         tokenized_len = len(tokenizer_protein_token(source[0]['value'], tokenizer))\n",
    "#         target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "#     return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def preprocess_plain(\n",
    "#     sources: Sequence[str],\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "# ) -> Dict:\n",
    "#     # add end signal and concatenate together\n",
    "#     conversations = []\n",
    "#     for source in sources:\n",
    "#         assert len(source) == 2\n",
    "#         assert DEFAULT_IMAGE_TOKEN in source[0]['value']\n",
    "#         source[0]['value'] = DEFAULT_IMAGE_TOKEN\n",
    "#         conversation = source[0]['value'] + source[1]['value'] + conversation_lib.default_conversation.sep\n",
    "#         conversations.append(conversation)\n",
    "#     # tokenize conversations\n",
    "#     input_ids = [tokenizer_image_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "#     targets = copy.deepcopy(input_ids)\n",
    "#     for target, source in zip(targets, sources):\n",
    "#         tokenized_len = len(tokenizer_image_token(source[0]['value'], tokenizer))\n",
    "#         target[:tokenized_len] = IGNORE_INDEX\n",
    "\n",
    "#     return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_llama_2_protein(sources, tokenizer, has_protein=True) -> Dict:\n",
    "    conv = default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "    conversations = []\n",
    "\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            source = source[1:]\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    if has_protein:\n",
    "        input_ids = torch.stack(\n",
    "            [tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(conversations, return_tensors=\"pt\", padding=\"longest\",\n",
    "                              max_length=tokenizer.model_max_length, truncation=True).input_ids\n",
    "\n",
    "    targets = input_ids.clone()\n",
    "    assert conv.sep_style == SeparatorStyle.LLAMA_2\n",
    "    sep = \"[/INST] \"\n",
    "\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if not rou:\n",
    "                break\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "\n",
    "            if has_protein:\n",
    "                round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "                instr_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 2\n",
    "            else:\n",
    "                round_len = len(tokenizer(rou).input_ids)\n",
    "                instr_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len:cur_len + instr_len] = IGNORE_INDEX\n",
    "            cur_len += round_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        # if cur_len != total_len:\n",
    "        #     target[:] = IGNORE_INDEX\n",
    "        #     print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}. (ignored)\")\n",
    "\n",
    "\n",
    "        if abs(cur_len - total_len) > 2:\n",
    "            print(f\"[W] Mismatch ignored: cur_len={cur_len}, total_len={total_len}\")\n",
    "\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "def preprocess_v1_protein(sources, tokenizer, has_protein=True) -> Dict:\n",
    "    conv = default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "    conversations = []\n",
    "\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            source = source[1:]\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    if has_protein:\n",
    "        input_ids = torch.stack(\n",
    "            [tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(conversations, return_tensors=\"pt\", padding=\"longest\",\n",
    "                              max_length=tokenizer.model_max_length, truncation=True).input_ids\n",
    "\n",
    "    targets = input_ids.clone()\n",
    "    assert conv.sep_style == SeparatorStyle.TWO\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        target[:cur_len] = IGNORE_INDEX\n",
    "\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if not rou:\n",
    "                break\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "\n",
    "            if has_protein:\n",
    "                round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "                instr_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 2\n",
    "            else:\n",
    "                round_len = len(tokenizer(rou).input_ids)\n",
    "                instr_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len:cur_len + instr_len] = IGNORE_INDEX\n",
    "            cur_len += round_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        # if cur_len != total_len:\n",
    "        #     target[:] = IGNORE_INDEX\n",
    "        #     print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}. (ignored)\")\n",
    "\n",
    "\n",
    "        if abs(cur_len - total_len) > 2:\n",
    "            print(f\"[W] Mismatch ignored: cur_len={cur_len}, total_len={total_len}\")\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "def preprocess_mpt_protein(sources, tokenizer, has_protein=True) -> Dict:\n",
    "    conv = default_conversation.copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "    conversations = []\n",
    "\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            source = source[1:]\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    if has_protein:\n",
    "        input_ids = torch.stack(\n",
    "            [tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations], dim=0\n",
    "        )\n",
    "    else:\n",
    "        input_ids = tokenizer(conversations, return_tensors=\"pt\", padding=\"longest\",\n",
    "                              max_length=tokenizer.model_max_length, truncation=True).input_ids\n",
    "\n",
    "    targets = input_ids.clone()\n",
    "    assert conv.sep_style == SeparatorStyle.MPT\n",
    "    sep = conv.sep + conv.roles[1]\n",
    "\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "        rounds = conversation.split(conv.sep)\n",
    "        re_rounds = [conv.sep.join(rounds[:3])]  # system + user + gpt\n",
    "        for k in range(3, len(rounds), 2):\n",
    "            re_rounds.append(conv.sep.join(rounds[k:k+2]))\n",
    "\n",
    "        cur_len = 0\n",
    "        for i, rou in enumerate(re_rounds):\n",
    "            if not rou:\n",
    "                break\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "\n",
    "            if has_protein:\n",
    "                round_len = len(tokenizer_protein_token(rou, tokenizer))\n",
    "                instr_len = len(tokenizer_protein_token(parts[0], tokenizer)) - 2\n",
    "            else:\n",
    "                round_len = len(tokenizer(rou).input_ids)\n",
    "                instr_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len:cur_len + instr_len] = IGNORE_INDEX\n",
    "            cur_len += round_len\n",
    "\n",
    "        target[cur_len:] = IGNORE_INDEX\n",
    "\n",
    "        # if cur_len != total_len:\n",
    "        #     target[:] = IGNORE_INDEX\n",
    "        #     print(f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}. (ignored)\")\n",
    "\n",
    "\n",
    "        if abs(cur_len - total_len) > 2:\n",
    "            print(f\"[W] Mismatch ignored: cur_len={cur_len}, total_len={total_len}\")\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "def preprocess_plain_protein(\n",
    "    sources: Sequence[Dict[str, str]],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        assert len(source) == 2\n",
    "        assert DEFAULT_SEQ_TOKEN in source[0]['value'] or DEFAULT_STR_TOKEN in source[0]['value'], \\\n",
    "            \"Expected <seq> or <str> in the input.\"\n",
    "\n",
    "        # Construct conversation string\n",
    "        conversation = source[0]['value'] + source[1]['value'] + default_conversation.sep\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    # Tokenize each prompt with protein tokenizer\n",
    "    input_ids = [tokenizer_protein_token(prompt, tokenizer, return_tensors='pt') for prompt in conversations]\n",
    "\n",
    "    # Clone to labels\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "\n",
    "    # Mask the prompt (first part) in the target\n",
    "    for target, source in zip(targets, sources):\n",
    "        prompt_len = len(tokenizer_protein_token(source[0]['value'], tokenizer))\n",
    "        target[:prompt_len] = IGNORE_INDEX\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _tokenize_fn(strings: Sequence[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "# The following code would more adpated for the protein-aware tokenizer.\n",
    "\n",
    "# def _tokenize_fn(strings: Sequence[str],\n",
    "#                  tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "#     \"\"\"Tokenize a list of strings using protein-aware tokenizer.\"\"\"\n",
    "#     input_ids = []\n",
    "#     input_ids_lens = []\n",
    "\n",
    "#     for text in strings:\n",
    "#         ids = tokenizer_protein_token(text, tokenizer, return_tensors='pt')\n",
    "#         input_ids.append(ids)\n",
    "#         input_ids_lens.append(ids.ne(tokenizer.pad_token_id).sum().item())\n",
    "\n",
    "#     return dict(\n",
    "#         input_ids=input_ids,\n",
    "#         labels=copy.deepcopy(input_ids),\n",
    "#         input_ids_lens=input_ids_lens,\n",
    "#         labels_lens=input_ids_lens,\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# def _mask_targets(target, tokenized_lens, speakers):\n",
    "#     cur_idx = tokenized_lens[0]  # system prompt\n",
    "#     target[:cur_idx] = IGNORE_INDEX\n",
    "#     tokenized_lens = tokenized_lens[1:]\n",
    "\n",
    "#     for tokenized_len, speaker in zip(tokenized_lens, speakers):\n",
    "#         if speaker == \"human\":\n",
    "#             target[cur_idx:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "#         cur_idx += tokenized_len\n",
    "\n",
    "#     target[cur_idx:] = IGNORE_INDEX  # mask remainder if any\n",
    "\n",
    "\n",
    "# def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "#     \"\"\"Add speaker tokens and signals to each sentence in the dialog.\"\"\"\n",
    "#     BEGIN_SIGNAL = \"### \"\n",
    "#     END_SIGNAL = \"\\n\"\n",
    "#     conversation = header\n",
    "\n",
    "#     for sentence in source:\n",
    "#         from_str = sentence[\"from\"].lower()\n",
    "#         if from_str == \"human\":\n",
    "#             from_str = default_conversation.roles[0]\n",
    "#         elif from_str == \"gpt\":\n",
    "#             from_str = default_conversation.roles[1]\n",
    "#         else:\n",
    "#             from_str = \"unknown\"\n",
    "\n",
    "#         # Normalize multimodal tokens if needed\n",
    "#         sentence[\"value\"] = sentence[\"value\"].replace(\"<SEQ>\", \"<seq>\").replace(\"<STR>\", \"<str>\")\n",
    "#         sentence[\"value\"] = BEGIN_SIGNAL + from_str + \": \" + sentence[\"value\"] + END_SIGNAL\n",
    "\n",
    "#         if get_conversation:\n",
    "#             conversation += sentence[\"value\"]\n",
    "\n",
    "#     conversation += BEGIN_SIGNAL\n",
    "#     return conversation\n",
    "\n",
    "\n",
    "def _mask_targets(target, tokenized_lens, speakers):\n",
    "    # cur_idx = 0\n",
    "    cur_idx = tokenized_lens[0]\n",
    "    tokenized_lens = tokenized_lens[1:]\n",
    "    target[:cur_idx] = IGNORE_INDEX\n",
    "    for tokenized_len, speaker in zip(tokenized_lens, speakers):\n",
    "        if speaker == \"human\":\n",
    "            target[cur_idx+2:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "        cur_idx += tokenized_len\n",
    "\n",
    "\n",
    "\n",
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "    \"\"\"Add speaker and start/end signal on each round.\"\"\"\n",
    "    BEGIN_SIGNAL = \"### \"\n",
    "    END_SIGNAL = \"\\n\"\n",
    "    conversation = header\n",
    "    for sentence in source:\n",
    "        from_str = sentence[\"from\"]\n",
    "        if from_str.lower() == \"human\":\n",
    "            from_str = default_conversation.roles[0]\n",
    "        elif from_str.lower() == \"gpt\":\n",
    "            from_str = default_conversation.roles[1]\n",
    "        else:\n",
    "            from_str = 'unknown'\n",
    "        sentence[\"value\"] = (BEGIN_SIGNAL + from_str + \": \" +\n",
    "                             sentence[\"value\"] + END_SIGNAL)\n",
    "        if get_conversation:\n",
    "            conversation += sentence[\"value\"]\n",
    "    conversation += BEGIN_SIGNAL\n",
    "    return conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    has_protein: bool = True  # replaces has_image\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "    \"\"\"\n",
    "    # Dispatch to conversation-style specific preprocessors\n",
    "    if default_conversation.sep_style == SeparatorStyle.PLAIN:\n",
    "        return preprocess_plain_protein(sources, tokenizer)\n",
    "\n",
    "    if default_conversation.sep_style == SeparatorStyle.LLAMA_2:\n",
    "        return preprocess_llama_2_protein(sources, tokenizer, has_protein=has_protein)\n",
    "\n",
    "    if default_conversation.version.startswith(\"v1\"):\n",
    "        return preprocess_v1_protein(sources, tokenizer, has_protein=has_protein)\n",
    "\n",
    "    if default_conversation.version == \"mpt\":\n",
    "        return preprocess_mpt_protein(sources, tokenizer, has_protein=has_protein)\n",
    "\n",
    "    # Fallback: generic instruction formatting + tokenization\n",
    "    conversations = []\n",
    "    for source in sources:\n",
    "        header = f\"{default_conversation.system}\\n\\n\"\n",
    "        conversation = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    # Tokenize full conversations\n",
    "    if has_protein:\n",
    "        input_ids = [\n",
    "            tokenizer_protein_token(prompt, tokenizer, return_tensors='pt')\n",
    "            for prompt in conversations\n",
    "        ]\n",
    "    else:\n",
    "        conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "        input_ids = conversations_tokenized[\"input_ids\"]\n",
    "\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "\n",
    "    for target, source in zip(targets, sources):\n",
    "        if has_protein:\n",
    "            # Compute token lengths for header and each utterance using protein-aware tokenizer\n",
    "            prompts = [f\"{default_conversation.system}\\n\\n\"] + [s[\"value\"] for s in source]\n",
    "            tokenized_lens = [\n",
    "                len(tokenizer_protein_token(p, tokenizer))\n",
    "                for p in prompts\n",
    "            ]\n",
    "        else:\n",
    "            tokenized_lens = _tokenize_fn(\n",
    "                [f\"{default_conversation.system}\\n\\n\"] + [s[\"value\"] for s in source],\n",
    "                tokenizer\n",
    "            )[\"input_ids_lens\"]\n",
    "\n",
    "        speakers = [s[\"from\"] for s in source]\n",
    "        _mask_targets(target, tokenized_lens, speakers)\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazySupervisedProteinDataset(Dataset):\n",
    "    \"\"\"Protein multimodal dataset for instruction tuning.\"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer,\n",
    "                 data_args,\n",
    "                 seq_tower=None,\n",
    "                 struc_tower=None):\n",
    "        super().__init__()\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.list_data_dict = [json.loads(line) for line in f if line.strip()]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_args = data_args\n",
    "        self.seq_tower = seq_tower  # Should have .tokenize()\n",
    "        self.struc_tower = struc_tower  # Should have .structure_processor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        return [\n",
    "            sum(len(conv['value'].split()) for conv in sample['conversations']) +\n",
    "            (128 if 'seq' in sample or 'str' in sample else 0)\n",
    "            for sample in self.list_data_dict\n",
    "        ]\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        lengths = []\n",
    "        for sample in self.list_data_dict:\n",
    "            base_len = sum(len(conv['value'].split()) for conv in sample['conversations'])\n",
    "            if 'seq' in sample or 'str' in sample:\n",
    "                lengths.append(base_len)\n",
    "            else:\n",
    "                lengths.append(-base_len)\n",
    "        return lengths\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.list_data_dict[idx]\n",
    "        conversations = copy.deepcopy(sample[\"conversations\"])\n",
    "        # Step 1: Insert <seq>/<str> tokens\n",
    "        sources = preprocess_multimodal([conversations], self.data_args)\n",
    "\n",
    "        # Step 2: Tokenize conversation for decoder\n",
    "        data_dict = preprocess(\n",
    "            sources,\n",
    "            self.tokenizer,\n",
    "            has_protein=('sequence' in sample or 'structure_path' in sample)\n",
    "        )\n",
    "        data_dict = {\n",
    "            \"input_ids\": data_dict[\"input_ids\"][0],\n",
    "            \"labels\": data_dict[\"labels\"][0]\n",
    "        }\n",
    "\n",
    "        # Step 3: Protein sequence processing\n",
    "        if \"sequence\" in sample and self.seq_tower is not None:\n",
    "            seq_tokenized = self.seq_tower.tokenize([sample[\"sequence\"]],\n",
    "                                                    return_tensors='pt', padding=True, truncation=True)\n",
    "            data_dict[\"seq_input_ids\"] = seq_tokenized[\"input_ids\"][0]\n",
    "            data_dict[\"seq_attention_mask\"] = seq_tokenized[\"attention_mask\"][0]\n",
    "\n",
    "        # Step 4: Structure preprocessing (L, 3, 3)\n",
    "        if \"structure_path\" in sample and self.struc_tower is not None:\n",
    "            try:\n",
    "                coords = self.struc_tower.structure_processor(\n",
    "                    sample[\"structure_path\"],\n",
    "                    chain=sample.get(\"structure_chain\", \"A\")\n",
    "                )\n",
    "                data_dict[\"struc_coords\"] = coords  # tensor, will be moved in collator\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Structure loading failed for idx {idx}: {e}\")\n",
    "                data_dict[\"struc_coords\"] = None\n",
    "        return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPISupervisedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    OPI-loading dataset for Pannot multimodal instruction tuning with sequence + structure.\n",
    "    Supports OPI-style data with keys: 'instruction', 'input' (sequence), 'output', and optionally 'structure'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer,\n",
    "                 data_args):\n",
    "        super().__init__()\n",
    "        with open(data_path, 'r') as f:\n",
    "            self.list_data_dict = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_args = data_args\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    @property\n",
    "    def lengths(self):\n",
    "        \"\"\"\n",
    "        Token lengths for each sample (approximate).\n",
    "        Includes fixed offset for <seq> and <str> placeholders.\n",
    "        \"\"\"\n",
    "        length_list = []\n",
    "        for sample in self.list_data_dict:\n",
    "            inst_len = len(sample['instruction'].split())\n",
    "            out_len = len(sample['output'].split())\n",
    "            modality_tokens = 0\n",
    "            if 'input' in sample:\n",
    "                modality_tokens += 4  # rough length for <seq>\n",
    "            if 'structure' in sample:\n",
    "                modality_tokens += 4  # rough length for <str>\n",
    "            length_list.append(inst_len + out_len + modality_tokens)\n",
    "        return length_list\n",
    "\n",
    "    @property\n",
    "    def modality_lengths(self):\n",
    "        \"\"\"\n",
    "        Used to group samples by modality in length-based batching.\n",
    "        All positive → indicates multimodal.\n",
    "        \"\"\"\n",
    "        return self.lengths\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        sample = self.list_data_dict[idx]\n",
    "        instruction = sample['instruction']\n",
    "        output = sample['output']\n",
    "        sequence = sample.get('input', None)\n",
    "        structure = sample.get('structure', None)\n",
    "\n",
    "        # Build multimodal-aware prompt with <seq> and/or <str>\n",
    "        prompt_parts = [instruction]\n",
    "        if sequence is not None:\n",
    "            prompt_parts.append(DEFAULT_SEQ_TOKEN)\n",
    "        if structure is not None:\n",
    "            prompt_parts.append(DEFAULT_STR_TOKEN)\n",
    "\n",
    "        prompt = '\\n'.join(prompt_parts)\n",
    "\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\": prompt},\n",
    "            {\"from\": \"gpt\", \"value\": output}\n",
    "        ]\n",
    "\n",
    "        # Inject multimodal wrappers\n",
    "        sources = preprocess_multimodal([conversation], self.data_args)\n",
    "\n",
    "        # Tokenize + label masking\n",
    "        processed = preprocess(\n",
    "            sources, self.tokenizer, has_protein=True\n",
    "        )\n",
    "\n",
    "        data_dict = {\n",
    "            \"input_ids\": processed[\"input_ids\"][0],\n",
    "            \"labels\": processed[\"labels\"][0]\n",
    "        }\n",
    "\n",
    "        if sequence is not None:\n",
    "            data_dict[\"sequence\"] = sequence\n",
    "        if structure is not None:\n",
    "            data_dict[\"structure\"] = structure\n",
    "\n",
    "        return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedProteinDataset(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [instance[\"input_ids\"] for instance in instances]\n",
    "        labels = [instance[\"labels\"] for instance in instances]\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        }\n",
    "\n",
    "        # Handle optional protein sequence features\n",
    "        if \"seq_input_ids\" in instances[0]:\n",
    "            seq_input_ids = [inst[\"seq_input_ids\"] for inst in instances]\n",
    "            seq_attention_mask = [inst[\"seq_attention_mask\"] for inst in instances]\n",
    "            batch[\"seq_input_ids\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "                seq_input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            batch[\"seq_attention_mask\"] = torch.nn.utils.rnn.pad_sequence(\n",
    "                seq_attention_mask, batch_first=True, padding_value=0\n",
    "            )\n",
    "\n",
    "        # Handle structure features (pad to max L)\n",
    "        if \"struc_coords\" in instances[0]:\n",
    "            coords_list = []\n",
    "            max_len = max((inst[\"struc_coords\"].shape[0] if inst[\"struc_coords\"] is not None else 0)\n",
    "                          for inst in instances)\n",
    "\n",
    "            for inst in instances:\n",
    "                coord = inst[\"struc_coords\"]\n",
    "                if coord is None:\n",
    "                    padded = torch.full((max_len, 3, 3), float(\"nan\"))\n",
    "                else:\n",
    "                    pad_len = max_len - coord.shape[0]\n",
    "                    padded = torch.nn.functional.pad(coord, (0, 0, 0, 0, 0, pad_len), value=float(\"nan\"))\n",
    "                coords_list.append(padded)\n",
    "\n",
    "            batch[\"struc_coords\"] = torch.stack(coords_list)\n",
    "\n",
    "        return batch\n",
    "# @dataclass\n",
    "# class DataCollatorForSupervisedProteinDataset:\n",
    "#     tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "#     def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "#         input_ids = [instance[\"input_ids\"] for instance in instances]\n",
    "#         labels = [instance[\"labels\"] for instance in instances]\n",
    "\n",
    "#         input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "#             input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "#         )\n",
    "#         labels = torch.nn.utils.rnn.pad_sequence(\n",
    "#             labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "#         )\n",
    "\n",
    "#         batch = {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"labels\": labels,\n",
    "#             \"attention_mask\": input_ids.ne(self.tokenizer.pad_token_id),\n",
    "#         }\n",
    "\n",
    "#         # Optional: keep raw sequence/structure for encoder use\n",
    "#         if \"sequence\" in instances[0]:\n",
    "#             batch[\"sequences\"] = [inst.get(\"sequence\", None) for inst in instances]\n",
    "\n",
    "#         if \"structure\" in instances[0]:\n",
    "#             batch[\"structures\"] = [inst.get(\"structure\", None) for inst in instances]\n",
    "\n",
    "#         return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForOPI:\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        # Token IDs & Labels\n",
    "        input_ids = [inst[\"input_ids\"] for inst in instances]\n",
    "        labels = [inst[\"labels\"] for inst in instances]\n",
    "\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(\n",
    "            labels, batch_first=True, padding_value=IGNORE_INDEX\n",
    "        )\n",
    "\n",
    "        # Truncate to max model length\n",
    "        input_ids = input_ids[:, :self.tokenizer.model_max_length]\n",
    "        labels = labels[:, :self.tokenizer.model_max_length]\n",
    "\n",
    "        batch = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        }\n",
    "\n",
    "        # Optional: keep raw sequence/structure for encoder use\n",
    "        if \"sequence\" in instances[0]:\n",
    "            batch[\"sequences\"] = [inst.get(\"sequence\", None) for inst in instances]\n",
    "\n",
    "        if \"structure\" in instances[0]:\n",
    "            batch[\"structures\"] = [inst.get(\"structure\", None) for inst in instances]\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised_data_module(tokenizer, data_args) -> Dict:\n",
    "    train_dataset = LazySupervisedProteinDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.data_path,\n",
    "        data_args=data_args,\n",
    "    )\n",
    "    data_collator = DataCollatorForSupervisedProteinDataset(tokenizer=tokenizer)\n",
    "    return dict(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_opi_supervised_data_module(tokenizer, data_args) -> Dict:\n",
    "    train_dataset = OPISupervisedDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=data_args.data_path,\n",
    "        data_args=data_args,\n",
    "    )\n",
    "    data_collator = DataCollatorForOPI(tokenizer=tokenizer)\n",
    "    return dict(\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=None,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, PretrainedConfig\n",
    "import torch.nn as nn\n",
    "class ESMSeqTower(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = 'facebook/esm2_t6_8M_UR50D',\n",
    "        args=None,\n",
    "        delay_load: bool = False,\n",
    "        no_pooling: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.is_loaded = False\n",
    "        self.model_name = model_name\n",
    "        self.args = args\n",
    "\n",
    "        self.select_layer = getattr(args, 'protein_select_layer', -1)\n",
    "        self.pooling = getattr(args, 'protein_pooling', 'cls')  # 'cls' or 'mean'\n",
    "        self.no_pooling = no_pooling\n",
    "\n",
    "        if not delay_load or getattr(args, 'unfreeze_mm_seq_tower', False):\n",
    "            self.load_model()\n",
    "\n",
    "    def load_model(self, device_map=None):\n",
    "        if self.is_loaded:\n",
    "            print(f'{self.model_name} is already loaded. Skipping load.')\n",
    "            return\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        self.encoder = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=True,\n",
    "            device_map=device_map\n",
    "        )\n",
    "        self.encoder.requires_grad_(False)\n",
    "        self.is_loaded = True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states[self.select_layer]\n",
    "\n",
    "        if self.no_pooling:\n",
    "            return hidden_states  # (B, L, D)\n",
    "\n",
    "        if self.pooling == 'cls':\n",
    "            return hidden_states[:, 0, :]  # (B, D)\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = attention_mask.unsqueeze(-1).expand_as(hidden_states)\n",
    "            sum_emb = torch.sum(hidden_states * mask, dim=1)\n",
    "            counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "            return sum_emb / counts\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling type: {self.pooling}\")\n",
    "\n",
    "    def tokenize(self, sequences, return_tensors='pt', padding=True, truncation=True, max_length=1024):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        return self.tokenizer(\n",
    "            sequences,\n",
    "            return_tensors=return_tensors,\n",
    "            padding=padding,\n",
    "            truncation=truncation,\n",
    "            max_length=max_length\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        if self.no_pooling:\n",
    "            return torch.zeros(1, 1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.encoder.dtype if self.is_loaded else torch.get_default_dtype()\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.encoder.parameters()).device if self.is_loaded else torch.device('cpu')\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self.encoder.config if self.is_loaded else PretrainedConfig.from_pretrained(self.model_name)\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load OPI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Step 1: Create dummy OPI-like test data ===\n",
    "# dummy_data = [\n",
    "#     {\n",
    "#         \"instruction\": \"What is the function of this protein?\",\n",
    "#         \"input\": \"MSEQNNTEMTFQIQRIYTKDISFEAPNAPHVFQKDWMA\",\n",
    "#         \"structure\": \"dummy_structure_info\",\n",
    "#         \"output\": \"It acts as a kinase inhibitor.\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# with NamedTemporaryFile(mode=\"w\", delete=False, suffix=\".json\") as f:\n",
    "#     json.dump(dummy_data, f)\n",
    "#     dummy_json_path = f.name\n",
    "opi_demo_path = \"/home/yining_yang/Documents/lm/Pannot/data/OPI_full_1.61M_train_first_10000.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the embeddings to the PannotLlamaForCausalLM\n",
    "# Load TinyLlama config and tokenizer\n",
    "\n",
    "# === Step 2: Define dummy tokenizer and args ===\n",
    "pretrained_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "# config = PannotConfig.from_pretrained(pretrained_model_name)\n",
    "\n",
    "tokenizer.add_tokens([DEFAULT_SEQ_TOKEN, DEFAULT_STR_TOKEN], special_tokens=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[910,\n",
       " 338,\n",
       " 263,\n",
       " 2107,\n",
       " 26823,\n",
       " 306,\n",
       " 864,\n",
       " 304,\n",
       " 6559,\n",
       " 29901,\n",
       " 29871,\n",
       " -330,\n",
       " 259,\n",
       " -360]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_protein_token(\"This is a great protein I want to study: <seq> <str>\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='TinyLlama/TinyLlama-1.1B-Chat-v1.0', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<seq>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<str>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seq_tower = ESMSeqTower(no_pooling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = SimpleNamespace(\n",
    "    data_path=opi_demo_path,\n",
    "    is_multimodal=True,\n",
    "    use_seq_start_end=False,\n",
    "    use_str_start_end=False\n",
    ")\n",
    "\n",
    "# === Step 3: Call make_supervised_data_module ===\n",
    "data_module = make_opi_supervised_data_module(tokenizer, data_args)\n",
    "dataset = data_module[\"train_dataset\"]\n",
    "collator = data_module[\"data_collator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Prompt:\", sample)\n",
    "# print(\"Tokenized:\", tokenizer(sample).input_ids)\n",
    "# print(\"Expected total len:\", sample)\n",
    "# print(\"Calculated cur_len:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Output ---\n",
      "input_ids: tensor([  319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082,\n",
      "        20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,   322,\n",
      "         1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155, 29889,\n",
      "         3148,  1001, 29901, 29871,  -330, 29871,    13, 23084,   919,   278,\n",
      "        13303, 29361,   310,   278,  1494, 26823, 15602,   319,  1799,  9047,\n",
      "        13566, 29901,   319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
      "         3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
      "         3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
      "          327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
      "        26533,     2])\n",
      "labels: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
      "         3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
      "         3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
      "          327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
      "        26533,     2])\n",
      "sequence: MHAVRYSQAFIPTLKEAPADAQVASHKLLVRAGFIRQLGAGIYDYLPLAKRSLAKVEAIVREEMDAIGGQEFYLPALHPAEIWKESGRWDVMGDNMFRLKDRKGGDYCLGMTHEEIFTAVARDELRSYRQLPQVWYQIQTKFRDEPRPKSGLLRVRQFTMKDAYSFDVDRAGLDRSYEDQRRAYEKIFTRCGLDFVAVQAHSGSMGGSESSEFMVRTDAGEDLVAACPRCRYAANTETATSRVAAEADGPGLGTPEKFATPGVVTIEALEQAPYSVAARRQLKTLVYMADEQPVIAVVRGDQELNEAKLQTATGAVAVRPAHPEEIPPLMGARAGSLGAVRFTRARVLLDPSLADRKDMVTGANEDGFHLRGVDVRRDVLAHGATLAELRTVKAGEGCPRCDGTLDVFKALEIGHIFKLGTKYSESMKATVLDAEGKQVPIVMGSYGIGVERILAAAIELHHDDNGIVFPMAIAPFHATVLTLGPEPELRKAAEEVVAALGKEGVEVLFDDRDERAGVKFKDADLLGIPIRIAVGKKGLAAGNVEWKLRKGGAVELVPVGEVARKAAEAVRAAT\n",
      "\n",
      "--- Collated Batch ---\n",
      "input_ids shape: torch.Size([1, 92])\n",
      "labels shape: torch.Size([1, 92])\n",
      "attention_mask shape: torch.Size([1, 92])\n",
      "seq_input_ids: [seq_input_ids not returned by collator]\n",
      "struc_coords: [struc_coords not returned by collator]\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Test a batch ===\n",
    "sample = dataset[0]\n",
    "print(\"\\n--- Sample Output ---\")\n",
    "print(\"input_ids:\", sample[\"input_ids\"])\n",
    "print(\"labels:\", sample[\"labels\"])\n",
    "print(\"sequence:\", sample[\"sequence\"])\n",
    "# print(\"structure:\", sample[\"structure\"])\n",
    "\n",
    "# Collate one batch\n",
    "batch = collator([sample])\n",
    "print(\"\\n--- Collated Batch ---\")\n",
    "print(\"input_ids shape:\", batch[\"input_ids\"].shape)\n",
    "print(\"labels shape:\", batch[\"labels\"].shape)\n",
    "print(\"attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"seq_input_ids:\", batch.get(\"seq_input_ids\", \"[seq_input_ids not returned by collator]\"))\n",
    "print(\"struc_coords:\", batch.get(\"struc_coords\", \"[struc_coords not returned by collator]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082,\n",
       "          20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,   322,\n",
       "           1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155, 29889,\n",
       "           3148,  1001, 29901, 29871,  -330, 29871,    13, 23084,   919,   278,\n",
       "          13303, 29361,   310,   278,  1494, 26823, 15602,   319,  1799,  9047,\n",
       "          13566, 29901,   319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
       "           3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
       "           3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
       "            327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
       "          26533,     2]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,   319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
       "           3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
       "           3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
       "            327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
       "          26533,     2]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False]]),\n",
       " 'sequences': ['MHAVRYSQAFIPTLKEAPADAQVASHKLLVRAGFIRQLGAGIYDYLPLAKRSLAKVEAIVREEMDAIGGQEFYLPALHPAEIWKESGRWDVMGDNMFRLKDRKGGDYCLGMTHEEIFTAVARDELRSYRQLPQVWYQIQTKFRDEPRPKSGLLRVRQFTMKDAYSFDVDRAGLDRSYEDQRRAYEKIFTRCGLDFVAVQAHSGSMGGSESSEFMVRTDAGEDLVAACPRCRYAANTETATSRVAAEADGPGLGTPEKFATPGVVTIEALEQAPYSVAARRQLKTLVYMADEQPVIAVVRGDQELNEAKLQTATGAVAVRPAHPEEIPPLMGARAGSLGAVRFTRARVLLDPSLADRKDMVTGANEDGFHLRGVDVRRDVLAHGATLAELRTVKAGEGCPRCDGTLDVFKALEIGHIFKLGTKYSESMKATVLDAEGKQVPIVMGSYGIGVERILAAAIELHHDDNGIVFPMAIAPFHATVLTLGPEPELRKAAEEVVAALGKEGVEVLFDDRDERAGVKFKDADLLGIPIRIAVGKKGLAAGNVEWKLRKGGAVELVPVGEVARKAAEAVRAAT']}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 21099, 919, 278, 13303, 29361, 310, 278, 1494, 26823, 15602]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Predict the functional keywords of the following protein sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: \""
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([  319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116, 21082,\n",
    "        20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,   322,\n",
    "         1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155, 29889,\n",
    "         3148,  1001, 29901, 29871])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPredict the functional keywords of the following protein sequences ASSISTANT: Aminoacyl-tRNA synthetase; ATP-binding; Cytoplasm; Ligase; Nucleotide-binding; Protein biosynthesis</s>'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([29871,    13, 23084,   919,   278,\n",
    "        13303, 29361,   310,   278,  1494, 26823, 15602,   319,  1799,  9047,\n",
    "        13566, 29901,   319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
    "         3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
    "         3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
    "          327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
    "        26533,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aminoacyl-tRNA synthetase; ATP-binding; Cytoplasm; Ligase; Nucleotide-binding; Protein biosynthesis</s>'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([319,  1195, 29877,   562,  2904, 29899, 29873, 29934,\n",
    "         3521, 14710,   300,   559, 29936, 27884, 29899, 19672, 29936,  8045,\n",
    "         3332,  3333, 29885, 29936, 21894,   559, 29936,   405,  1682,   280,\n",
    "          327,   680, 29899, 19672, 29936, 14409,   262,   289,  2363,   948,\n",
    "        26533,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"conversations\": [\n",
    "#     {\"from\": \"human\", \"value\": \"Here is a protein: <seq> and its structure: <str>. What is its function?\"},\n",
    "#     {\"from\": \"gpt\", \"value\": \"This protein is likely involved in...\" }\n",
    "#   ],\n",
    "#   \"seq_feat\": [ ... ],  // 1D or 2D float array from ESM2\n",
    "#   \"str_feat\": [ ... ]   // 1D or 2D float array from ESM-IF1\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
